{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94e11c2-f096-4835-a913-54fde624614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 07:59:16.356715: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762239556.372613    3013 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762239556.377714    3013 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762239556.390426    3013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762239556.390454    3013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762239556.390456    3013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762239556.390458    3013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-04 07:59:16.395116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Model, layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c145a6-2a14-4a10-bd9c-734de29c7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Convert to numpy float32\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Normalize data between 0 and 1\n",
    "x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "# splitting Data\n",
    "x_train, valid_data, y_train, valid_labels = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size = 0.25,\n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c15af8d-c091-4711-b780-da86e673731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762239561.121778    3013 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Checking data\n",
    "\n",
    "x_train = tf.pad(x_train, paddings=[[0, 0], [2, 2], [2, 2]], mode=\"CONSTANT\", constant_values=0)\n",
    "x_test = tf.pad(x_test, paddings=[[0, 0], [2, 2], [2, 2]], mode=\"CONSTANT\", constant_values=0)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8f1985-560f-42d3-9de2-ed67c6b0f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330fe0e8-6920-47b4-9648-ed2768b78634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "total_samples = len(x_train)\n",
    "\n",
    "train_size = int(0.75 * len(x_train))\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 125\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "steps_per_epoch = int(np.ceil(total_samples / batch_size))\n",
    "\n",
    "conv_layer1 = 6\n",
    "conv_layer2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07953a8-d1d0-453d-b115-9e98393219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data batching, splitting and shuffle\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(5000).repeat()\n",
    "\n",
    "#train_data = train_data.take(train_size)\n",
    "#valid_data = train_data.skip(train_size)\n",
    "\n",
    "train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "#valid_data = valid_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b41ee076-b743-4db5-9ddd-d454dc1e4469",
   "metadata": {},
   "source": [
    "# Defining weight and bias\n",
    "# Weights\n",
    "W = {\n",
    "    \"w1\": tf.Variable(tf.random.normal([1, 6]), , name=\"weight1\"),\n",
    "    \"w2\": tf.Variable(tf.random.normal([6, 16]), name=\"weight2\"),\n",
    "    \"w3\": tf.Variable(tf.random.normal([5*5*16, 120]), name=\"weight3\"),\n",
    "    \"w4\": tf.Variable(tf.random.normal([120, 84]), name=\"weight4\"),\n",
    "    \"w5\": tf.Variable(tf.random.normal([84, 10]), name=\"weight5\")\n",
    "}\n",
    "                      \n",
    "\n",
    "# Bias\n",
    "B = {\n",
    "    \"b1\": tf.Variable(tf.zeros([6]), name=\"bias1\"),\n",
    "    \"b2\": tf.Variable(tf.zeros([16]), name=\"bias2\"),\n",
    "    \"b3\": tf.Variable(tf.zeros([120]), name=\"bias3\"),\n",
    "    \"b4\": tf.Variable(tf.zeros([84]), name=\"bias4\"),\n",
    "    \"b4\": tf.Variable(tf.zeros([10]), name=\"bias5\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea54de6c-c99a-4788-a4b6-4fde376219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model\n",
    "class LeNet(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # Convolutional Layer 1\n",
    "        self.w1 = tf.Variable(tf.random.normal([3, 3, 1, conv_layer1]), name=\"weight1\", trainable=True)\n",
    "        self.b1 = tf.Variable(tf.zeros([conv_layer1]), name=\"bias1\", trainable=True)\n",
    "\n",
    "        # Convolutional Layer 2\n",
    "        self.w2 = tf.Variable(tf.random.normal([3, 3, conv_layer1, conv_layer2]), name=\"weight2\", trainable=True)\n",
    "        self.b2 = tf.Variable(tf.zeros([conv_layer2]), name=\"bias2\", trainable=True)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        self.w3 = tf.Variable(tf.random.normal([5*5*conv_layer2, 120]), name=\"weight3\", trainable=True)\n",
    "        self.b3 = tf.Variable(tf.zeros([120]), name=\"bias3\", trainable=True)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        self.w4 = tf.Variable(tf.random.normal([120, 84]), name=\"weight4\", trainable=True)\n",
    "        self.b4 = tf.Variable(tf.zeros([84]), name=\"bias4\", trainable=True)\n",
    "\n",
    "        # Output Layer\n",
    "        self.w5 = tf.Variable(tf.random.normal([84, 10]), name=\"weight5\", trainable=True)\n",
    "        self.b5 = tf.Variable(tf.zeros([10]), name=\"bias5\", trainable=True)\n",
    "\n",
    "        \n",
    "    def conv2d(self, x, filter_W, bias_conv, stride=2, padding=\"VALID\"):\n",
    "        conv_layer = tf.nn.conv2d(\n",
    "            x, filter_W,\n",
    "            strides = [1, stride, stride, 1],\n",
    "            padding = padding\n",
    "        )\n",
    "        conv_layer = tf.nn.bias_add(conv_layer, bias_conv)\n",
    "        conv_layer = tf.nn.relu(conv_layer)\n",
    "        return conv_layer\n",
    "\n",
    "    def maxpool2d(self, x, k=2):\n",
    "        return tf.nn.max_pool(x, k=[1, k, k, 1], strides=[1, k, k, 1], padding=\"SAME\")\n",
    "    \n",
    "    def __call__(self, x, is_training=False):\n",
    "        \n",
    "        # Layer 1\n",
    "        conv1 = self.conv2d(x, self.w1, self.b1)\n",
    "        conv1 = self.maxpool2d(conv1, k=2)\n",
    "\n",
    "        # Layer 2\n",
    "        conv2 = self.conv2d(conv1, self.w2, self.b2)\n",
    "        conv2 = self.maxpool2d(conv2, k=2)\n",
    "\n",
    "        # Flatten Layer \n",
    "        flatten = tf.reshape(conv2, [-1, self.w3.get_shape().as_list(0)[0]])\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        fc1 = tf.nn.bias_add(tf.matmul(flatten, self.w3), self.b3)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        # Fullly connected layer 2\n",
    "        fc2 = tf.nn.bias_add(tf.matmul(fc1, self.w4), self.b4)\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "        # Output layer\n",
    "        out = tf.nn.bias_add(tf.matmul(fc2, self.w5), self.b5)\n",
    "\n",
    "        if not is_training:\n",
    "            return tf.nn.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3796678-ad0a-4339-93d8-4c62f0ac3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4578bc6d-45b7-4a3b-8750-52fc907fa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, y_true):\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=x)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(pred, y_true):\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e89cc2-0686-466e-a36b-ddc4249952d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(X, Y):\n",
    "    with tf.GradientTape() as g:\n",
    "        logits = model(X, is_training=True)\n",
    "        loss = cross_entropy(logits, Y)\n",
    "\n",
    "    gradient = g.gradients(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(gradient, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7841c45e-d5c7-4b40-9e0c-8a8aa32856f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (_TupleWrapper((<tf.Variable 'weight1:0' shape=(3, 3, 1, 6) dtype=float32, numpy=\narray([[[[-1.0434712 , -0.12306508,  0.7636716 , -0.73300934,\n          -1.2034198 , -1.1468683 ]],\n\n        [[-1.3701671 ,  0.2196709 ,  0.481947  ,  0.667011  ,\n          -0.6440105 , -0.1546383 ]],\n\n        [[ 0.23313454, -0.05384781, -1.4789863 , -1.7302068 ,\n          -0.51457494,  1.3431426 ]]],\n\n\n       [[[ 0.29648885,  1.0678004 ,  0.8219487 , -0.8838747 ,\n           0.33531335, -0.9067335 ]],\n\n        [[-1.8494409 ,  1.2814767 ,  0.15951508,  0.68538016,\n           0.07038898, -1.4757422 ]],\n\n        [[-0.7577252 ,  0.57884985,  0.70002645, -0.11738571,\n           0.35614645,  0.79097   ]]],\n\n\n       [[[-0.27882332, -1.6731254 ,  0.959114  , -0.6997842 ,\n           0.30457515,  0.6272917 ]],\n\n        [[-0.7665482 ,  0.7495768 ,  0.24067816, -0.01117166,\n          -0.02441178,  0.13706195]],\n\n        [[ 1.1451366 ,  0.04250425, -1.9980545 , -0.7088995 ,\n          -0.7577295 ,  1.6958355 ]]]], dtype=float32)>,))) with an unsupported type (<class 'tensorflow.python.trackable.data_structures._TupleWrapper'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs + \u001b[32m1\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step, (batch_X, batch_Y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_data.take(steps_per_epoch), \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[43mrun_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_Y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     train_pred = model(batch_X, is_training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m     train_loss = cross_entropy(train_pred, batch_Y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mrun_optimization\u001b[39m\u001b[34m(X, Y)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_optimization\u001b[39m(X, Y):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m g:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m         loss = cross_entropy(logits, Y)\n\u001b[32m      6\u001b[39m     gradient = g.gradients(loss, model.trainable_variables)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mLeNet.__call__\u001b[39m\u001b[34m(self, x, is_training)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, is_training=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Layer 1\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     conv1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mb1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     conv1 = \u001b[38;5;28mself\u001b[39m.maxpool2d(conv1, k=\u001b[32m2\u001b[39m)\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Layer 2\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mLeNet.conv2d\u001b[39m\u001b[34m(self, x, filter_W, bias_conv, stride, padding)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconv2d\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, filter_W, bias_conv, stride=\u001b[32m2\u001b[39m, padding=\u001b[33m\"\u001b[39m\u001b[33mVALID\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     conv_layer = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_W\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     conv_layer = tf.nn.bias_add(conv_layer, bias_conv)\n\u001b[32m     34\u001b[39m     conv_layer = tf.nn.relu(conv_layer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m     dtype = dtypes.as_dtype(dtype).as_datatype_enum\n\u001b[32m    107\u001b[39m ctx.ensure_initialized()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Attempt to convert a value (_TupleWrapper((<tf.Variable 'weight1:0' shape=(3, 3, 1, 6) dtype=float32, numpy=\narray([[[[-1.0434712 , -0.12306508,  0.7636716 , -0.73300934,\n          -1.2034198 , -1.1468683 ]],\n\n        [[-1.3701671 ,  0.2196709 ,  0.481947  ,  0.667011  ,\n          -0.6440105 , -0.1546383 ]],\n\n        [[ 0.23313454, -0.05384781, -1.4789863 , -1.7302068 ,\n          -0.51457494,  1.3431426 ]]],\n\n\n       [[[ 0.29648885,  1.0678004 ,  0.8219487 , -0.8838747 ,\n           0.33531335, -0.9067335 ]],\n\n        [[-1.8494409 ,  1.2814767 ,  0.15951508,  0.68538016,\n           0.07038898, -1.4757422 ]],\n\n        [[-0.7577252 ,  0.57884985,  0.70002645, -0.11738571,\n           0.35614645,  0.79097   ]]],\n\n\n       [[[-0.27882332, -1.6731254 ,  0.959114  , -0.6997842 ,\n           0.30457515,  0.6272917 ]],\n\n        [[-0.7665482 ,  0.7495768 ,  0.24067816, -0.01117166,\n          -0.02441178,  0.13706195]],\n\n        [[ 1.1451366 ,  0.04250425, -1.9980545 , -0.7088995 ,\n          -0.7577295 ,  1.6958355 ]]]], dtype=float32)>,))) with an unsupported type (<class 'tensorflow.python.trackable.data_structures._TupleWrapper'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs + 1):\n",
    "    for step, (batch_X, batch_Y) in enumerate(train_data.take(steps_per_epoch), 1):\n",
    "        run_optimization(batch_X, batch_Y)\n",
    "\n",
    "    train_pred = model(batch_X, is_training=True)\n",
    "    train_loss = cross_entropy(train_pred, batch_Y)\n",
    "    train_acc = accuracy(train_pred, batch_Y)\n",
    "    valid_pred = model(valid_data, is_training=True)\n",
    "    valid_loss = cross_entropy(valid_pred, valid_labels)\n",
    "    valid_acc = accuracy(valid_pred, valid_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch}, train_loss {train_loss}, train_accuracy{train_acc}, val_loss{val_loss}, val_accuracy{val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb02b2-6be7-43b0-b6b9-d03f2362bd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
