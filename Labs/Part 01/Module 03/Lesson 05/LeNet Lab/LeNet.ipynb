{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94e11c2-f096-4835-a913-54fde624614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:00:00.618152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762243200.639358   19481 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762243200.645601   19481 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762243200.663322   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762243200.663349   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762243200.663352   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762243200.663354   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-04 09:00:00.670389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Model, layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c145a6-2a14-4a10-bd9c-734de29c7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Convert to numpy float32\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Normalize data between 0 and 1\n",
    "x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test  = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "\n",
    "# splitting Data\n",
    "x_train, valid_data, y_train, valid_labels = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size = 0.25,\n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c15af8d-c091-4711-b780-da86e673731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762243206.565917   19481 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Checking data\n",
    "\n",
    "x_train = tf.pad(x_train, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "valid_data = tf.pad(valid_data, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "x_test = tf.pad(x_test, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8f1985-560f-42d3-9de2-ed67c6b0f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330fe0e8-6920-47b4-9648-ed2768b78634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "total_samples = len(x_train)\n",
    "\n",
    "train_size = int(0.75 * len(x_train))\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 125\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "steps_per_epoch = int(np.ceil(total_samples / batch_size))\n",
    "\n",
    "conv_layer1 = 6\n",
    "conv_layer2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07953a8-d1d0-453d-b115-9e98393219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data batching, splitting and shuffle\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(5000).repeat()\n",
    "\n",
    "\n",
    "train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea54de6c-c99a-4788-a4b6-4fde376219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model\n",
    "class LeNet(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # Convolutional Layer 1\n",
    "        self.w1 = tf.Variable(tf.random.normal([5, 5, 1, conv_layer1]), name=\"weight1\", trainable=True)\n",
    "        self.b1 = tf.Variable(tf.zeros([conv_layer1]), name=\"bias1\", trainable=True)\n",
    "\n",
    "        # Convolutional Layer 2\n",
    "        self.w2 = tf.Variable(tf.random.normal([5, 5, conv_layer1, conv_layer2]), name=\"weight2\", trainable=True)\n",
    "        self.b2 = tf.Variable(tf.zeros([conv_layer2]), name=\"bias2\", trainable=True)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        self.w3 = tf.Variable(tf.random.normal([5*5*conv_layer2, 120]), name=\"weight3\", trainable=True)\n",
    "        self.b3 = tf.Variable(tf.zeros([120]), name=\"bias3\", trainable=True)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        self.w4 = tf.Variable(tf.random.normal([120, 84]), name=\"weight4\", trainable=True)\n",
    "        self.b4 = tf.Variable(tf.zeros([84]), name=\"bias4\", trainable=True)\n",
    "\n",
    "        # Output Layer\n",
    "        self.w5 = tf.Variable(tf.random.normal([84, 10]), name=\"weight5\", trainable=True)\n",
    "        self.b5 = tf.Variable(tf.zeros([10]), name=\"bias5\", trainable=True)\n",
    "\n",
    "        \n",
    "    def conv2d(self, x, filter_W, bias_conv, stride=2, padding=\"VALID\"):\n",
    "        conv_layer = tf.nn.conv2d(\n",
    "            x, filter_W,\n",
    "            strides = [1, stride, stride, 1],\n",
    "            padding = padding\n",
    "        )\n",
    "        conv_layer = tf.nn.bias_add(conv_layer, bias_conv)\n",
    "        conv_layer = tf.nn.relu(conv_layer)\n",
    "        return conv_layer\n",
    "\n",
    "    def maxpool2d(self, x, k=2, s=2):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "    \n",
    "    def __call__(self, x, is_training=False):\n",
    "        \n",
    "        # Layer 1\n",
    "        conv1 = self.conv2d(x, self.w1, self.b1, 1)\n",
    "        conv1 = self.maxpool2d(conv1, 2, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        conv2 = self.conv2d(conv1, self.w2, self.b2, 1)\n",
    "        conv2 = self.maxpool2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten Layer \n",
    "        flatten = tf.reshape(conv2, [-1, self.w3.get_shape().as_list()[0]])\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        fc1 = tf.nn.bias_add(tf.matmul(flatten, self.w3), self.b3)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        # Fullly connected layer 2\n",
    "        fc2 = tf.nn.bias_add(tf.matmul(fc1, self.w4), self.b4)\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "        # Output layer\n",
    "        out = tf.nn.bias_add(tf.matmul(fc2, self.w5), self.b5)\n",
    "\n",
    "        if not is_training:\n",
    "            return tf.nn.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3796678-ad0a-4339-93d8-4c62f0ac3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4578bc6d-45b7-4a3b-8750-52fc907fa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, y_true):\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(pred, y_true):\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40e89cc2-0686-466e-a36b-ddc4249952d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(X, Y):\n",
    "    with tf.GradientTape() as g:\n",
    "        logits = model(X, is_training=True)\n",
    "        loss = cross_entropy(logits, Y)\n",
    "\n",
    "    gradient = g.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7841c45e-d5c7-4b40-9e0c-8a8aa32856f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762243208.212602   19481 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-11-04 09:00:40.493281: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss 290.3529357910156, train_accuracy 0.7279999852180481, validation_loss 451.6348876953125, validation_accuracy 0.7337999939918518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:01:12.380611: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss 227.10470581054688, train_accuracy 0.8320000171661377, validation_loss 232.46783447265625, validation_accuracy 0.8129333257675171\n",
      "Epoch 3, train_loss 64.57658386230469, train_accuracy 0.9039999842643738, validation_loss 157.64808654785156, validation_accuracy 0.8482666611671448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:02:21.631719: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train_loss 54.00155258178711, train_accuracy 0.8799999952316284, validation_loss 116.32801818847656, validation_accuracy 0.8668000102043152\n",
      "Epoch 5, train_loss 58.847015380859375, train_accuracy 0.871999979019165, validation_loss 92.23439025878906, validation_accuracy 0.8814666867256165\n",
      "Epoch 6, train_loss 62.29881286621094, train_accuracy 0.8640000224113464, validation_loss 79.09906005859375, validation_accuracy 0.8908666372299194\n",
      "Epoch 7, train_loss 47.945281982421875, train_accuracy 0.9359999895095825, validation_loss 68.80928039550781, validation_accuracy 0.9001333117485046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:04:16.067168: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, train_loss 45.096988677978516, train_accuracy 0.9120000004768372, validation_loss 58.150508880615234, validation_accuracy 0.9050666689872742\n",
      "Epoch 9, train_loss 9.306318283081055, train_accuracy 0.9520000219345093, validation_loss 51.075801849365234, validation_accuracy 0.911466658115387\n",
      "Epoch 10, train_loss 10.138213157653809, train_accuracy 0.9520000219345093, validation_loss 46.30019760131836, validation_accuracy 0.9155333042144775\n",
      "Epoch 11, train_loss 20.19655990600586, train_accuracy 0.9279999732971191, validation_loss 41.322349548339844, validation_accuracy 0.9177333116531372\n",
      "Epoch 12, train_loss 22.27643585205078, train_accuracy 0.9440000057220459, validation_loss 38.139732360839844, validation_accuracy 0.9213333129882812\n",
      "Epoch 13, train_loss 17.441896438598633, train_accuracy 0.9520000219345093, validation_loss 34.837650299072266, validation_accuracy 0.9233333468437195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_19481/2730138022.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;28;01min\u001b[39;00m range(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step, (batch_X, batch_Y) \u001b[38;5;28;01min\u001b[39;00m enumerate(train_data.take(steps_per_epoch), \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         run_optimization(batch_X, batch_Y)\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     train_pred = model(batch_X, is_training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m     train_loss = cross_entropy(train_pred, batch_Y)\n",
      "\u001b[32m/tmp/ipykernel_19481/3221404474.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X, Y)\u001b[39m\n\u001b[32m      3\u001b[39m         logits = model(X, is_training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m         loss = cross_entropy(logits, Y)\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m     gradient = g.gradient(loss, model.trainable_variables)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_gradients(self, grads_and_vars):\n\u001b[32m    462\u001b[39m         grads, trainable_variables = zip(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m         self.apply(grads, trainable_variables)\n\u001b[32m    464\u001b[39m         \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    465\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._iterations\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    523\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    524\u001b[39m                     grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;28;01min\u001b[39;00m grads]\n\u001b[32m    525\u001b[39m \n\u001b[32m    526\u001b[39m                 \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m                 self._backend_apply_gradients(grads, trainable_variables)\n\u001b[32m    528\u001b[39m                 \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    529\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;28;01min\u001b[39;00m trainable_variables:\n\u001b[32m    530\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m variable.constraint \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    589\u001b[39m             grads = self._clip_gradients(grads)\n\u001b[32m    590\u001b[39m             self._apply_weight_decay(trainable_variables)\n\u001b[32m    591\u001b[39m \n\u001b[32m    592\u001b[39m             \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m             self._backend_update_step(\n\u001b[32m    594\u001b[39m                 grads, trainable_variables, self.learning_rate\n\u001b[32m    595\u001b[39m             )\n\u001b[32m    596\u001b[39m \n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m trainable_variables\n\u001b[32m    117\u001b[39m         ]\n\u001b[32m    118\u001b[39m         grads_and_vars = list(zip(grads, trainable_variables))\n\u001b[32m    119\u001b[39m         grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[32m    121\u001b[39m             self._distributed_tf_update_step,\n\u001b[32m    122\u001b[39m             self._distribution_strategy,\n\u001b[32m    123\u001b[39m             grads_and_vars,\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m   Returns:\n\u001b[32m     48\u001b[39m     The \u001b[38;5;28;01mreturn\u001b[39;00m value of the `fn` call.\n\u001b[32m     49\u001b[39m   \"\"\"\n\u001b[32m     50\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, *args, **kwargs)\n\u001b[32m     52\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     return distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;28;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m             distribution.extended.update(\n\u001b[32m    135\u001b[39m                 var,\n\u001b[32m    136\u001b[39m                 apply_grad_to_update_var,\n\u001b[32m    137\u001b[39m                 args=(grad, learning_rate),\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3001\u001b[39m         _get_default_replica_context()):\n\u001b[32m   3002\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[32m   3004\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._update(var, fn, args, kwargs, group)\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m       return self._replica_ctx_update(\n\u001b[32m   3008\u001b[39m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update(self, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m     \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m     \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m       result = fn(*args, **kwargs)\n\u001b[32m   4082\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   4084\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/optimizers/adam.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    131\u001b[39m             v = v_hat\n\u001b[32m    132\u001b[39m         self.assign_sub(\n\u001b[32m    133\u001b[39m             variable,\n\u001b[32m    134\u001b[39m             ops.divide(\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m                 ops.multiply(m, alpha), ops.add(ops.sqrt(v), self.epsilon)\n\u001b[32m    136\u001b[39m             ),\n\u001b[32m    137\u001b[39m         )\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/ops/numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    233\u001b[39m            [10 12]], shape=(2, 2), dtype=int32)\n\u001b[32m    234\u001b[39m     \"\"\"\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m    236\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Add().symbolic_call(x1, x2)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.numpy.add(x1, x2)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/backend/tensorflow/sparse.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    489\u001b[39m                     x1 = tf.convert_to_tensor(x1)\n\u001b[32m    490\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m isinstance(x2, tf.IndexedSlices):\n\u001b[32m    491\u001b[39m                 \u001b[38;5;66;03m# x2 is an IndexedSlices, densify.\u001b[39;00m\n\u001b[32m    492\u001b[39m                 x2 = tf.convert_to_tensor(x2)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(x1, x2)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/keras/src/backend/tensorflow/numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(x2.shape) > \u001b[32m1\u001b[39m:\n\u001b[32m    128\u001b[39m             x2 = tf.squeeze(x2)\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.nn.bias_add(x1, x2, data_format=data_format)\n\u001b[32m    130\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.add(x1, x2)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m    143\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m    145\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m   3957\u001b[39m     y = ops.convert_to_tensor(y, dtype_hint=x.dtype.base_dtype, name=\u001b[33m\"y\"\u001b[39m)\n\u001b[32m   3958\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.dtype == dtypes.string:\n\u001b[32m   3959\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.add(x, y, name=name)\n\u001b[32m   3960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3961\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.add_v2(x, y, name=name)\n",
      "\u001b[32m~/miniconda3/envs/sdc/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m    478\u001b[39m         _ctx, \"AddV2\", name, x, y)\n\u001b[32m    479\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    481\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m    483\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    485\u001b[39m       return add_v2_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    for step, (batch_X, batch_Y) in enumerate(train_data.take(steps_per_epoch), 1):\n",
    "        run_optimization(batch_X, batch_Y)\n",
    "\n",
    "    train_pred = model(batch_X, is_training=True)\n",
    "    train_loss = cross_entropy(train_pred, batch_Y)\n",
    "    train_acc = accuracy(train_pred, batch_Y)\n",
    "    valid_pred = model(valid_data, is_training=True)\n",
    "    valid_loss = cross_entropy(valid_pred, valid_labels)\n",
    "    valid_acc = accuracy(valid_pred, valid_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch}, train_loss {train_loss}, train_accuracy {train_acc}, validation_loss {valid_loss}, validation_accuracy {valid_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d99ddc6-004e-4f85-92fc-9b1818d7490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 34.41279220581055 Test accuracy 0.9289000034332275\n"
     ]
    }
   ],
   "source": [
    "test_pred = model(x_test, is_training=True)\n",
    "test_loss = cross_entropy(test_pred, y_test)\n",
    "test_acc = accuracy(test_pred, y_test)\n",
    "\n",
    "print(f\"Test loss {test_loss} Test accuracy {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab3e103-11cf-4df1-8cbb-9257c263c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e9a1ea7-9917-4c1d-902b-b6b5479c435a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9289.0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc * len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496c95a-b380-4685-be9b-7c266caee03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
