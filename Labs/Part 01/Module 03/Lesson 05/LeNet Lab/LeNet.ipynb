{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94e11c2-f096-4835-a913-54fde624614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:00:00.618152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762243200.639358   19481 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762243200.645601   19481 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762243200.663322   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762243200.663349   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762243200.663352   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762243200.663354   19481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-04 09:00:00.670389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Model, layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c145a6-2a14-4a10-bd9c-734de29c7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Convert to numpy float32\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Normalize data between 0 and 1\n",
    "x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test  = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "\n",
    "# splitting Data\n",
    "x_train, valid_data, y_train, valid_labels = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size = 0.25,\n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c15af8d-c091-4711-b780-da86e673731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762243206.565917   19481 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Checking data\n",
    "\n",
    "x_train = tf.pad(x_train, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "valid_data = tf.pad(valid_data, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "x_test = tf.pad(x_test, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\", constant_values=0)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8f1985-560f-42d3-9de2-ed67c6b0f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330fe0e8-6920-47b4-9648-ed2768b78634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "total_samples = len(x_train)\n",
    "\n",
    "train_size = int(0.75 * len(x_train))\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 125\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "steps_per_epoch = int(np.ceil(total_samples / batch_size))\n",
    "\n",
    "conv_layer1 = 6\n",
    "conv_layer2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07953a8-d1d0-453d-b115-9e98393219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data batching, splitting and shuffle\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(5000).repeat()\n",
    "\n",
    "\n",
    "train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea54de6c-c99a-4788-a4b6-4fde376219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model\n",
    "class LeNet(tf.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # Convolutional Layer 1\n",
    "        self.w1 = tf.Variable(tf.random.normal([5, 5, 1, conv_layer1]), name=\"weight1\", trainable=True)\n",
    "        self.b1 = tf.Variable(tf.zeros([conv_layer1]), name=\"bias1\", trainable=True)\n",
    "\n",
    "        # Convolutional Layer 2\n",
    "        self.w2 = tf.Variable(tf.random.normal([5, 5, conv_layer1, conv_layer2]), name=\"weight2\", trainable=True)\n",
    "        self.b2 = tf.Variable(tf.zeros([conv_layer2]), name=\"bias2\", trainable=True)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        self.w3 = tf.Variable(tf.random.normal([5*5*conv_layer2, 120]), name=\"weight3\", trainable=True)\n",
    "        self.b3 = tf.Variable(tf.zeros([120]), name=\"bias3\", trainable=True)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        self.w4 = tf.Variable(tf.random.normal([120, 84]), name=\"weight4\", trainable=True)\n",
    "        self.b4 = tf.Variable(tf.zeros([84]), name=\"bias4\", trainable=True)\n",
    "\n",
    "        # Output Layer\n",
    "        self.w5 = tf.Variable(tf.random.normal([84, 10]), name=\"weight5\", trainable=True)\n",
    "        self.b5 = tf.Variable(tf.zeros([10]), name=\"bias5\", trainable=True)\n",
    "\n",
    "        \n",
    "    def conv2d(self, x, filter_W, bias_conv, stride=2, padding=\"VALID\"):\n",
    "        conv_layer = tf.nn.conv2d(\n",
    "            x, filter_W,\n",
    "            strides = [1, stride, stride, 1],\n",
    "            padding = padding\n",
    "        )\n",
    "        conv_layer = tf.nn.bias_add(conv_layer, bias_conv)\n",
    "        conv_layer = tf.nn.relu(conv_layer)\n",
    "        return conv_layer\n",
    "\n",
    "    def maxpool2d(self, x, k=2, s=2):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "    \n",
    "    def __call__(self, x, is_training=False):\n",
    "        \n",
    "        # Layer 1\n",
    "        conv1 = self.conv2d(x, self.w1, self.b1, 1)\n",
    "        conv1 = self.maxpool2d(conv1, 2, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        conv2 = self.conv2d(conv1, self.w2, self.b2, 1)\n",
    "        conv2 = self.maxpool2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten Layer \n",
    "        flatten = tf.reshape(conv2, [-1, self.w3.get_shape().as_list()[0]])\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        fc1 = tf.nn.bias_add(tf.matmul(flatten, self.w3), self.b3)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        # Fullly connected layer 2\n",
    "        fc2 = tf.nn.bias_add(tf.matmul(fc1, self.w4), self.b4)\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "        # Output layer\n",
    "        out = tf.nn.bias_add(tf.matmul(fc2, self.w5), self.b5)\n",
    "\n",
    "        if not is_training:\n",
    "            return tf.nn.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3796678-ad0a-4339-93d8-4c62f0ac3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4578bc6d-45b7-4a3b-8750-52fc907fa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, y_true):\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(pred, y_true):\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40e89cc2-0686-466e-a36b-ddc4249952d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(X, Y):\n",
    "    with tf.GradientTape() as g:\n",
    "        logits = model(X, is_training=True)\n",
    "        loss = cross_entropy(logits, Y)\n",
    "\n",
    "    gradient = g.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841c45e-d5c7-4b40-9e0c-8a8aa32856f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762243208.212602   19481 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-11-04 09:00:40.493281: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss 290.3529357910156, train_accuracy 0.7279999852180481, validation_loss 451.6348876953125, validation_accuracy 0.7337999939918518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:01:12.380611: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss 227.10470581054688, train_accuracy 0.8320000171661377, validation_loss 232.46783447265625, validation_accuracy 0.8129333257675171\n",
      "Epoch 3, train_loss 64.57658386230469, train_accuracy 0.9039999842643738, validation_loss 157.64808654785156, validation_accuracy 0.8482666611671448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 09:02:21.631719: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train_loss 54.00155258178711, train_accuracy 0.8799999952316284, validation_loss 116.32801818847656, validation_accuracy 0.8668000102043152\n",
      "Epoch 5, train_loss 58.847015380859375, train_accuracy 0.871999979019165, validation_loss 92.23439025878906, validation_accuracy 0.8814666867256165\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    for step, (batch_X, batch_Y) in enumerate(train_data.take(steps_per_epoch), 1):\n",
    "        run_optimization(batch_X, batch_Y)\n",
    "\n",
    "    train_pred = model(batch_X, is_training=True)\n",
    "    train_loss = cross_entropy(train_pred, batch_Y)\n",
    "    train_acc = accuracy(train_pred, batch_Y)\n",
    "    valid_pred = model(valid_data, is_training=True)\n",
    "    valid_loss = cross_entropy(valid_pred, valid_labels)\n",
    "    valid_acc = accuracy(valid_pred, valid_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch}, train_loss {train_loss}, train_accuracy {train_acc}, validation_loss {valid_loss}, validation_accuracy {valid_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99ddc6-004e-4f85-92fc-9b1818d7490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model(x_test, is_training=True)\n",
    "test_loss = cross_entropy(test_pred, y_test)\n",
    "test_acc = accuracy(test_pred, y_test)\n",
    "\n",
    "print(f\"Test loss {test_loss} Test accuracy {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3e103-11cf-4df1-8cbb-9257c263c24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
