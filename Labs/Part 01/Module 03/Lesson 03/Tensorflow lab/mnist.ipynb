{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6d08fe36-d4fa-4292-a54c-ccc02f156920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d08fe36-d4fa-4292-a54c-ccc02f156920",
        "outputId": "c963cb18-2ab3-400d-ad14-f2d7da882356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data and modules loaded.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Load the modules\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reload the data\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  pickle_data = pickle.load(f)\n",
        "  train_features = pickle_data['train_dataset']\n",
        "  train_labels = pickle_data['train_labels']\n",
        "  valid_features = pickle_data['valid_dataset']\n",
        "  valid_labels = pickle_data['valid_labels']\n",
        "  test_features = pickle_data['test_dataset']\n",
        "  test_labels = pickle_data['test_labels']\n",
        "  del pickle_data  # Free up memory\n",
        "\n",
        "\n",
        "print('Data and modules loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e49cb51f-2576-43b1-a76a-fa642597976d",
      "metadata": {
        "id": "e49cb51f-2576-43b1-a76a-fa642597976d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data has been normalized\n",
        "total_samples = len(train_features)\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "num_output = 10\n",
        "epochs = 40\n",
        "batch_size = 256\n",
        "steps_per_epoch = int(np.ceil(total_samples / batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2dd74ff5-8247-4304-a21e-227e1dc40857",
      "metadata": {
        "id": "2dd74ff5-8247-4304-a21e-227e1dc40857"
      },
      "outputs": [],
      "source": [
        "# Cleaning Data\n",
        "#valid_labels, test_labels = np.array(valid_labels, np.int64), np.array(test_labels, np.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "val_features, test_features = np.array(valid_features, np.float32), np.array(test_features, np.float32)\n",
        "\n",
        "# Data has been normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "52a9b1dc-03ab-4628-ac52-544a0540979a",
      "metadata": {
        "id": "52a9b1dc-03ab-4628-ac52-544a0540979a"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aa753673-88d7-4601-9dff-f9ade3381a24",
      "metadata": {
        "id": "aa753673-88d7-4601-9dff-f9ade3381a24",
        "outputId": "7203a3a7-5779-4804-a420-a3956f08b953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142500\n"
          ]
        }
      ],
      "source": [
        "features = train_features.shape[1]\n",
        "print(total_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bdc4dec7-5917-4ff5-941e-3d82566529ac",
      "metadata": {
        "id": "bdc4dec7-5917-4ff5-941e-3d82566529ac"
      },
      "outputs": [],
      "source": [
        "#W = tf.Variable(tf.random.normal([features, num_output], name=\"weights1\"))\n",
        "#B = tf.Variable(tf.zeros([num_output], name=\"bias1\"))\n",
        "mu = 0\n",
        "sigma = 0.001\n",
        "\n",
        "W = {\n",
        "    \"w1\": tf.Variable(tf.random.normal([features, 128], mean=mu, stddev=sigma), name=\"weight1\"),\n",
        "    \"w2\": tf.Variable(tf.random.normal([128, 32], mean=mu, stddev=sigma), name=\"weight2\"),\n",
        "    #\"w3\": tf.Variable(tf.random.normal([128, 32], mean=mu, stddev=sigma), name=\"weight3\"),\n",
        "    \"out\": tf.Variable(tf.random.normal([32, num_output], mean=mu, stddev=sigma), name=\"weight_out\")\n",
        "}\n",
        "\n",
        "B = {\n",
        "    \"b1\": tf.Variable(tf.zeros([128], name=\"bias1\")),\n",
        "    \"b2\": tf.Variable(tf.zeros([32], name=\"bias2\")),\n",
        "    #\"b3\": tf.Variable(tf.zeros([32], name=\"bias3\")),\n",
        "    \"out\": tf.Variable(tf.zeros([num_output], name=\"bias_out\"))\n",
        "}\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Model\n",
        "class MyModel(tf.Module):\n",
        "    def __call__(self, X):\n",
        "      X = tf.add(tf.matmul(X, W[\"w1\"]), B[\"b1\"])\n",
        "      X = tf.nn.relu(X)\n",
        "      X = tf.add(tf.matmul(X, W[\"w2\"]), B[\"b2\"])\n",
        "      X = tf.nn.relu(X)\n",
        "      #X = tf.add(tf.matmul(X, W[\"w3\"]), B[\"b3\"])\n",
        "      #X = tf.nn.relu(X)\n",
        "      return tf.nn.softmax(tf.add(tf.matmul(X, W[\"out\"]), B[\"out\"]))\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    # It has been one-hot encoded before storing as pickle\n",
        "    #y_true = tf.one_hot(y_true, depth=num_output)\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), 1))\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
        "        y_true = tf.argmax(y_true, axis=1)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "def run_optimizer(X, Y):\n",
        "    with tf.GradientTape() as g:\n",
        "        logit = model(X)\n",
        "        loss = cross_entropy(logit, Y)\n",
        "\n",
        "    trainable_variables = list(W.values()) + list(B.values())\n",
        "\n",
        "    gradients = g.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return None\n",
        "\n",
        "def batch_data(X, Y, batch_size):\n",
        "    output_data = []\n",
        "    sample_size = len(X)\n",
        "    for step in range(0, sample_size, batch_size):\n",
        "        start = batch_size * step\n",
        "        end = batch_size + start\n",
        "        batch_X = X[start:end]\n",
        "        batch_Y = Y[start:end]\n",
        "        yield batch_X, batch_Y\n",
        "\n",
        "model=MyModel()\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model = model)\n",
        "checkpoint.save(\"./checkpoints/mymodel\")\n",
        "manager = tf.train.CheckpointManager(checkpoint, \"./checkpoints\", max_to_keep=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eed6bd8f-4bdc-4110-8cec-91ba172b3b55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eed6bd8f-4bdc-4110-8cec-91ba172b3b55",
        "outputId": "c9679753-dc2c-49ef-ef95-66e020d75536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, train_loss 0.4027448892593384, train_accuracy 0.890625, val_loss 0.5390413999557495, val_accuracy 0.8420000076293945\n",
            "Epoch: 2, train_loss 0.5751396417617798, train_accuracy 0.8203125, val_loss 0.4794931709766388, val_accuracy 0.8555999994277954\n",
            "Epoch: 3, train_loss 0.42961764335632324, train_accuracy 0.84375, val_loss 0.4768078327178955, val_accuracy 0.8589333295822144\n",
            "Epoch: 4, train_loss 0.5140156149864197, train_accuracy 0.83984375, val_loss 0.4674294888973236, val_accuracy 0.8622666597366333\n",
            "Epoch: 5, train_loss 0.33536675572395325, train_accuracy 0.8984375, val_loss 0.4455438554286957, val_accuracy 0.8665333390235901\n",
            "Epoch: 6, train_loss 0.3073121905326843, train_accuracy 0.9140625, val_loss 0.42466306686401367, val_accuracy 0.8704000115394592\n",
            "Epoch: 7, train_loss 0.4008389115333557, train_accuracy 0.8828125, val_loss 0.424173504114151, val_accuracy 0.8733333349227905\n",
            "Epoch: 8, train_loss 0.31074684858322144, train_accuracy 0.90625, val_loss 0.42650213837623596, val_accuracy 0.8709333539009094\n",
            "Epoch: 9, train_loss 0.31201446056365967, train_accuracy 0.88671875, val_loss 0.41194769740104675, val_accuracy 0.8750666379928589\n",
            "Epoch: 10, train_loss 0.306374728679657, train_accuracy 0.90234375, val_loss 0.4075741469860077, val_accuracy 0.878000020980835\n",
            "Epoch: 11, train_loss 0.3474709987640381, train_accuracy 0.8984375, val_loss 0.41459372639656067, val_accuracy 0.874666690826416\n",
            "Epoch: 12, train_loss 0.2069731205701828, train_accuracy 0.9296875, val_loss 0.40253064036369324, val_accuracy 0.8802666664123535\n",
            "Epoch: 13, train_loss 0.3529227674007416, train_accuracy 0.88671875, val_loss 0.41008567810058594, val_accuracy 0.880133330821991\n",
            "Epoch: 14, train_loss 0.36420121788978577, train_accuracy 0.875, val_loss 0.41013240814208984, val_accuracy 0.8795999884605408\n",
            "Epoch: 15, train_loss 0.37998273968696594, train_accuracy 0.90234375, val_loss 0.3996724486351013, val_accuracy 0.8833333253860474\n",
            "Epoch: 16, train_loss 0.26403743028640747, train_accuracy 0.921875, val_loss 0.4022602438926697, val_accuracy 0.8791999816894531\n",
            "Epoch: 17, train_loss 0.2714115381240845, train_accuracy 0.9140625, val_loss 0.3973073661327362, val_accuracy 0.8840000033378601\n",
            "Epoch: 18, train_loss 0.2916536331176758, train_accuracy 0.9140625, val_loss 0.3959577977657318, val_accuracy 0.881600022315979\n",
            "Epoch: 19, train_loss 0.2927156686782837, train_accuracy 0.91015625, val_loss 0.3953872323036194, val_accuracy 0.8855999708175659\n",
            "Epoch: 20, train_loss 0.3072657585144043, train_accuracy 0.8984375, val_loss 0.40085452795028687, val_accuracy 0.8846666812896729\n",
            "Epoch: 21, train_loss 0.28317514061927795, train_accuracy 0.8984375, val_loss 0.392473042011261, val_accuracy 0.8857333064079285\n",
            "Epoch: 22, train_loss 0.2609572410583496, train_accuracy 0.9140625, val_loss 0.3901614546775818, val_accuracy 0.8862666487693787\n",
            "Epoch: 23, train_loss 0.2993181049823761, train_accuracy 0.90234375, val_loss 0.39496859908103943, val_accuracy 0.887066662311554\n",
            "Epoch: 24, train_loss 0.34207791090011597, train_accuracy 0.8984375, val_loss 0.38758769631385803, val_accuracy 0.8876000046730042\n",
            "Epoch: 25, train_loss 0.21978504955768585, train_accuracy 0.92578125, val_loss 0.3909132480621338, val_accuracy 0.88919997215271\n",
            "Epoch: 26, train_loss 0.3254411816596985, train_accuracy 0.8828125, val_loss 0.395017147064209, val_accuracy 0.890666663646698\n",
            "Epoch: 27, train_loss 0.3383423686027527, train_accuracy 0.8984375, val_loss 0.38780495524406433, val_accuracy 0.892133355140686\n",
            "Epoch: 28, train_loss 0.34841540455818176, train_accuracy 0.875, val_loss 0.4015107750892639, val_accuracy 0.8909333348274231\n",
            "Epoch: 29, train_loss 0.1839158684015274, train_accuracy 0.921875, val_loss 0.4021608531475067, val_accuracy 0.8899999856948853\n",
            "Epoch: 30, train_loss 0.21991929411888123, train_accuracy 0.921875, val_loss 0.40236029028892517, val_accuracy 0.8920000195503235\n",
            "Epoch: 31, train_loss 0.25156956911087036, train_accuracy 0.90234375, val_loss 0.39755454659461975, val_accuracy 0.8926666378974915\n",
            "Epoch: 32, train_loss 0.3148542642593384, train_accuracy 0.8671875, val_loss 0.4037233591079712, val_accuracy 0.8941333293914795\n",
            "Epoch: 33, train_loss 0.26266616582870483, train_accuracy 0.90625, val_loss 0.39659565687179565, val_accuracy 0.8918666839599609\n",
            "Epoch: 34, train_loss 0.24813193082809448, train_accuracy 0.9140625, val_loss 0.3983887732028961, val_accuracy 0.8931999802589417\n",
            "Epoch: 35, train_loss 0.26270127296447754, train_accuracy 0.921875, val_loss 0.3945648670196533, val_accuracy 0.8941333293914795\n",
            "Epoch: 36, train_loss 0.21291367709636688, train_accuracy 0.921875, val_loss 0.40366634726524353, val_accuracy 0.8918666839599609\n",
            "Epoch: 37, train_loss 0.17999683320522308, train_accuracy 0.9296875, val_loss 0.40250301361083984, val_accuracy 0.8939999938011169\n",
            "Epoch: 38, train_loss 0.18783968687057495, train_accuracy 0.9453125, val_loss 0.39759284257888794, val_accuracy 0.8918666839599609\n",
            "Epoch: 39, train_loss 0.20322418212890625, train_accuracy 0.94140625, val_loss 0.4060637354850769, val_accuracy 0.8941333293914795\n",
            "Epoch: 40, train_loss 0.17796741425991058, train_accuracy 0.94140625, val_loss 0.41179102659225464, val_accuracy 0.8930666446685791\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for step, (batch_X, batch_Y) in enumerate(train_data.take(steps_per_epoch), 1):\n",
        "        run_optimizer(batch_X, batch_Y)\n",
        "\n",
        "    val_pred = model(valid_features)\n",
        "    val_loss = cross_entropy(val_pred, valid_labels)\n",
        "    val_acc = accuracy(val_pred, valid_labels)\n",
        "    train_pred = model(batch_X)\n",
        "    train_loss = cross_entropy(train_pred, batch_Y)\n",
        "    train_acc = accuracy(train_pred, batch_Y)\n",
        "    manager.save()\n",
        "    new_learning_rate = learning_rate * (0.95 ** epoch)\n",
        "    optimizer.learning_rate.assign(new_learning_rate)\n",
        "\n",
        "\n",
        "    print(f\"Epoch: {epoch}, train_loss {train_loss}, train_accuracy {train_acc}, val_loss {val_loss}, val_accuracy {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "89964352-f865-4417-afa5-26bfef13f437",
      "metadata": {
        "id": "89964352-f865-4417-afa5-26bfef13f437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba8cff1-1681-4c5e-ea0a-04c88e4974f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_loss 0.2845991551876068, Test_accuracy 0.9287999868392944\n"
          ]
        }
      ],
      "source": [
        "test_pred = model(test_features)\n",
        "test_loss = cross_entropy(test_pred, test_labels)\n",
        "test_acc = accuracy(test_pred, test_labels)\n",
        "print(f\"Test_loss {test_loss}, Test_accuracy {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbaa8601-7cee-442a-a667-c1e849ca20ce",
      "metadata": {
        "id": "cbaa8601-7cee-442a-a667-c1e849ca20ce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}