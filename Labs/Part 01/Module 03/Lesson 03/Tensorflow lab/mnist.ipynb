{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import os\n",
        "import pickle\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import resample\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print('All modules imported.')\n",
        "\n",
        "def download(url, file):\n",
        "    \"\"\"\n",
        "    Download file from <url>\n",
        "    :param url: URL to file\n",
        "    :param file: Local file path\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file):\n",
        "        print('Downloading ' + file + '...')\n",
        "        urlretrieve(url, file)\n",
        "        print('Download Finished')\n",
        "\n",
        "# Download the training and test dataset.\n",
        "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
        "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
        "\n",
        "# Make sure the files aren't corrupted\n",
        "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
        "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
        "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
        "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
        "\n",
        "# Wait until you see that all files have been downloaded.\n",
        "print('All files downloaded.')\n",
        "\n",
        "def uncompress_features_labels(file):\n",
        "    \"\"\"\n",
        "    Uncompress features and labels from a zip file\n",
        "    :param file: The zip file to extract the data from\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with ZipFile(file) as zipf:\n",
        "        # Progress Bar\n",
        "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
        "\n",
        "        # Get features and labels from all files\n",
        "        for filename in filenames_pbar:\n",
        "            # Check if the file is a directory\n",
        "            if not filename.endswith('/'):\n",
        "                with zipf.open(filename) as image_file:\n",
        "                    image = Image.open(image_file)\n",
        "                    image.load()\n",
        "                    # Load image data as 1 dimensional array\n",
        "                    # We're using float32 to save on memory space\n",
        "                    feature = np.array(image, dtype=np.float32).flatten()\n",
        "\n",
        "                # Get the the letter from the filename.  This is the letter of the image.\n",
        "                label = os.path.split(filename)[1][0]\n",
        "\n",
        "                features.append(feature)\n",
        "                labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Get the features and labels from the zip files\n",
        "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
        "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
        "\n",
        "# Limit the amount of data to work with a docker container\n",
        "docker_size_limit = 150000\n",
        "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
        "\n",
        "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
        "is_features_normal = False\n",
        "is_labels_encod = False\n",
        "\n",
        "# Wait until you see that all features and labels have been uncompressed.\n",
        "print('All features and labels uncompressed.')\n",
        "\n",
        "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
        "def normalize_grayscale(image_data):\n",
        "    \"\"\"\n",
        "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
        "    :param image_data: The image data to be normalized\n",
        "    :return: Normalized image data\n",
        "    \"\"\"\n",
        "    # TODO: Implement Min-Max scaling for grayscale image data\n",
        "    a, b = 0.1, 0.9\n",
        "    b_a = b - a\n",
        "    min_data = 0\n",
        "    max_data = 255\n",
        "    data_diff = max_data - min_data\n",
        "    return a + ((image_data - min_data) * b_a)/(max_data - min_data)\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# Test Cases\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
        "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
        "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
        "    decimal=3)\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
        "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
        "     0.896862745098, 0.9])\n",
        "\n",
        "if not is_features_normal:\n",
        "    train_features = normalize_grayscale(train_features)\n",
        "    test_features = normalize_grayscale(test_features)\n",
        "    is_features_normal = True\n",
        "\n",
        "print('Tests Passed!')\n",
        "\n",
        "if not is_labels_encod:\n",
        "    # Turn labels into numbers and apply One-Hot Encoding\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(train_labels)\n",
        "    train_labels = encoder.transform(train_labels)\n",
        "    test_labels = encoder.transform(test_labels)\n",
        "\n",
        "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
        "    train_labels = train_labels.astype(np.float32)\n",
        "    test_labels = test_labels.astype(np.float32)\n",
        "    is_labels_encod = True\n",
        "\n",
        "print('Labels One-Hot Encoded')\n",
        "\n",
        "assert is_features_normal, 'You skipped the step to normalize the features'\n",
        "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
        "\n",
        "# Get randomized datasets for training and validation\n",
        "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    test_size=0.05,\n",
        "    random_state=832289)\n",
        "\n",
        "print('Training features and labels randomized and split.')\n",
        "\n",
        "# Save the data for easy access\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "if not os.path.isfile(pickle_file):\n",
        "    print('Saving data to pickle file...')\n",
        "    try:\n",
        "        with open('notMNIST.pickle', 'wb') as pfile:\n",
        "            pickle.dump(\n",
        "                {\n",
        "                    'train_dataset': train_features,\n",
        "                    'train_labels': train_labels,\n",
        "                    'valid_dataset': valid_features,\n",
        "                    'valid_labels': valid_labels,\n",
        "                    'test_dataset': test_features,\n",
        "                    'test_labels': test_labels,\n",
        "                },\n",
        "                pfile, pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "        print('Unable to save data to', pickle_file, ':', e)\n",
        "        raise\n",
        "\n",
        "print('Data cached in pickle file.')"
      ],
      "metadata": {
        "id": "plLp_rJBvrtB",
        "outputId": "f62a3753-1530-482a-e863-bab41d4c7c98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "plLp_rJBvrtB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All modules imported.\n",
            "All files downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 210001/210001 [00:49<00:00, 4271.56files/s]\n",
            "100%|██████████| 10001/10001 [00:01<00:00, 6174.27files/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All features and labels uncompressed.\n",
            "Tests Passed!\n",
            "Labels One-Hot Encoded\n",
            "Training features and labels randomized and split.\n",
            "Data cached in pickle file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d08fe36-d4fa-4292-a54c-ccc02f156920",
      "metadata": {
        "id": "6d08fe36-d4fa-4292-a54c-ccc02f156920",
        "outputId": "061f7b92-b15a-4172-8716-b9283c9f52db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data and modules loaded.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Load the modules\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reload the data\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  pickle_data = pickle.load(f)\n",
        "  train_features = pickle_data['train_dataset']\n",
        "  train_labels = pickle_data['train_labels']\n",
        "  valid_features = pickle_data['valid_dataset']\n",
        "  valid_labels = pickle_data['valid_labels']\n",
        "  test_features = pickle_data['test_dataset']\n",
        "  test_labels = pickle_data['test_labels']\n",
        "  del pickle_data  # Free up memory\n",
        "\n",
        "\n",
        "print('Data and modules loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e49cb51f-2576-43b1-a76a-fa642597976d",
      "metadata": {
        "id": "e49cb51f-2576-43b1-a76a-fa642597976d"
      },
      "outputs": [],
      "source": [
        "# Data has been normalized\n",
        "total_samples = len(train_features)\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.00\n",
        "num_output = 10\n",
        "epochs = 100\n",
        "batch_size = 500\n",
        "steps_per_epoch = int(np.ceil(total_samples / batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2dd74ff5-8247-4304-a21e-227e1dc40857",
      "metadata": {
        "id": "2dd74ff5-8247-4304-a21e-227e1dc40857"
      },
      "outputs": [],
      "source": [
        "# Cleaning Data\n",
        "#valid_labels, test_labels = np.array(valid_labels, np.int64), np.array(test_labels, np.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "val_features, test_features = np.array(valid_features, np.float32), np.array(test_features, np.float32)\n",
        "\n",
        "# Data has been normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "52a9b1dc-03ab-4628-ac52-544a0540979a",
      "metadata": {
        "id": "52a9b1dc-03ab-4628-ac52-544a0540979a"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "aa753673-88d7-4601-9dff-f9ade3381a24",
      "metadata": {
        "id": "aa753673-88d7-4601-9dff-f9ade3381a24"
      },
      "outputs": [],
      "source": [
        "features = train_features.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bdc4dec7-5917-4ff5-941e-3d82566529ac",
      "metadata": {
        "id": "bdc4dec7-5917-4ff5-941e-3d82566529ac"
      },
      "outputs": [],
      "source": [
        "W = tf.Variable(tf.random.normal([features, num_output], name=\"weights1\"))\n",
        "B = tf.Variable(tf.zeros([num_output], name=\"bias1\"))\n",
        "\n",
        "W = {\n",
        "    \"w1\": tf.Variable(tf.random.normal([features, num_output])),\n",
        "    \"w2\": tf.Variable(tf.random.normal([features, num_output])),\n",
        "    \"out\": tf.Variable(tf.random.normal([features, num_output]))\n",
        "}\n",
        "\n",
        "B = {\n",
        "    \"b1\": tf.Variable(tf.zeros([num_output], name=\"bias1\")),\n",
        "    \"b2\": tf.Variable(tf.zeros([num_output], name=\"bias1\")),\n",
        "    \"out\": tf.Variable(tf.zeros([num_output], name=\"bias1\"))\n",
        "}\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Model\n",
        "class MyModel(tf.Module):\n",
        "    def __call__(self, X):\n",
        "      X = tf.add(tf.matmul(X, W[\"w1\"]), B[\"b1\"])\n",
        "      X = tf.nn.relu(X)\n",
        "      X = tf.add(tf.matmul(X, W[\"w2\"]), B[\"b2\"])\n",
        "      X = tf.nn.relu(X)\n",
        "      return tf.nn.softmax(tf.add(tf.matmul(X, W[\"out\"]), B[\"out\"]))\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    # It has been one-hot encoded before storing as pickle\n",
        "    #y_true = tf.one_hot(y_true, depth=num_output)\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), 1))\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
        "        y_true = tf.argmax(y_true, axis=1)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "def run_optimizer(X, Y):\n",
        "    with tf.GradientTape() as g:\n",
        "        logit = model(X)\n",
        "        loss = cross_entropy(logit, Y)\n",
        "\n",
        "    trainable_variables = list(W.values()) + list(B.values())\n",
        "\n",
        "    gradients = g.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return None\n",
        "\n",
        "def batch_data(X, Y, batch_size):\n",
        "    output_data = []\n",
        "    sample_size = len(X)\n",
        "    for step in range(0, sample_size, batch_size):\n",
        "        start = batch_size * step\n",
        "        end = batch_size + start\n",
        "        batch_X = X[start:end]\n",
        "        batch_Y = Y[start:end]\n",
        "        yield batch_X, batch_Y\n",
        "\n",
        "model=MyModel()\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model = model)\n",
        "checkpoint.save(\"./checkpoints/mymodel\")\n",
        "manager = tf.train.CheckpointManager(checkpoint, \"./checkpoints\", max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eed6bd8f-4bdc-4110-8cec-91ba172b3b55",
      "metadata": {
        "id": "eed6bd8f-4bdc-4110-8cec-91ba172b3b55",
        "outputId": "ba812152-f871-4c75-9a24-575932a80d6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, train_loss 11.253149032592773, train_accuracy 0.15600000321865082, val_loss 10.816572189331055, val_accuracy 0.16973333060741425\n",
            "Epoch: 2, train_loss 7.536696434020996, train_accuracy 0.24400000274181366, val_loss 8.032686233520508, val_accuracy 0.24586667120456696\n",
            "Epoch: 3, train_loss 6.552656650543213, train_accuracy 0.32600000500679016, val_loss 6.25577974319458, val_accuracy 0.3322666585445404\n",
            "Epoch: 4, train_loss 4.819737434387207, train_accuracy 0.43799999356269836, val_loss 5.18454122543335, val_accuracy 0.40066665410995483\n",
            "Epoch: 5, train_loss 4.864267349243164, train_accuracy 0.44600000977516174, val_loss 4.458240985870361, val_accuracy 0.4575999975204468\n",
            "Epoch: 6, train_loss 3.90141224861145, train_accuracy 0.5260000228881836, val_loss 3.968876838684082, val_accuracy 0.49959999322891235\n",
            "Epoch: 7, train_loss 3.571348190307617, train_accuracy 0.5099999904632568, val_loss 3.6327245235443115, val_accuracy 0.5332000255584717\n",
            "Epoch: 8, train_loss 3.881819725036621, train_accuracy 0.5299999713897705, val_loss 3.3946633338928223, val_accuracy 0.5633333325386047\n",
            "Epoch: 9, train_loss 2.830399990081787, train_accuracy 0.6079999804496765, val_loss 3.2191643714904785, val_accuracy 0.5871999859809875\n",
            "Epoch: 10, train_loss 2.68904447555542, train_accuracy 0.6259999871253967, val_loss 3.0812504291534424, val_accuracy 0.6033333539962769\n",
            "Epoch: 11, train_loss 2.8405306339263916, train_accuracy 0.6299999952316284, val_loss 2.9708151817321777, val_accuracy 0.6169333457946777\n",
            "Epoch: 12, train_loss 3.153299570083618, train_accuracy 0.6140000224113464, val_loss 2.879007339477539, val_accuracy 0.6294666528701782\n",
            "Epoch: 13, train_loss 2.320596694946289, train_accuracy 0.6819999814033508, val_loss 2.800388813018799, val_accuracy 0.6398666501045227\n",
            "Epoch: 14, train_loss 2.8696975708007812, train_accuracy 0.656000018119812, val_loss 2.733684539794922, val_accuracy 0.6478666663169861\n",
            "Epoch: 15, train_loss 2.6147258281707764, train_accuracy 0.6800000071525574, val_loss 2.674919843673706, val_accuracy 0.6537333130836487\n",
            "Epoch: 16, train_loss 2.6404831409454346, train_accuracy 0.6499999761581421, val_loss 2.6231210231781006, val_accuracy 0.6601333618164062\n",
            "Epoch: 17, train_loss 2.643902063369751, train_accuracy 0.6600000262260437, val_loss 2.5771310329437256, val_accuracy 0.6669333577156067\n",
            "Epoch: 18, train_loss 2.529597282409668, train_accuracy 0.6779999732971191, val_loss 2.5347843170166016, val_accuracy 0.6740000247955322\n",
            "Epoch: 19, train_loss 2.766836643218994, train_accuracy 0.6700000166893005, val_loss 2.4972310066223145, val_accuracy 0.6782666444778442\n",
            "Epoch: 20, train_loss 2.2368180751800537, train_accuracy 0.7260000109672546, val_loss 2.4631710052490234, val_accuracy 0.6830666661262512\n",
            "Epoch: 21, train_loss 2.0368573665618896, train_accuracy 0.7179999947547913, val_loss 2.4311861991882324, val_accuracy 0.6880000233650208\n",
            "Epoch: 22, train_loss 2.5595810413360596, train_accuracy 0.6840000152587891, val_loss 2.4023642539978027, val_accuracy 0.6917333602905273\n",
            "Epoch: 23, train_loss 2.384470224380493, train_accuracy 0.7039999961853027, val_loss 2.375044584274292, val_accuracy 0.6948000192642212\n",
            "Epoch: 24, train_loss 2.415778398513794, train_accuracy 0.6899999976158142, val_loss 2.349599838256836, val_accuracy 0.6970666646957397\n",
            "Epoch: 25, train_loss 2.4329493045806885, train_accuracy 0.6940000057220459, val_loss 2.3257648944854736, val_accuracy 0.699999988079071\n",
            "Epoch: 26, train_loss 2.2357630729675293, train_accuracy 0.7039999961853027, val_loss 2.303781747817993, val_accuracy 0.7016000151634216\n",
            "Epoch: 27, train_loss 2.6661171913146973, train_accuracy 0.6700000166893005, val_loss 2.2830305099487305, val_accuracy 0.7041333317756653\n",
            "Epoch: 28, train_loss 2.192368268966675, train_accuracy 0.7279999852180481, val_loss 2.2631893157958984, val_accuracy 0.7058666944503784\n",
            "Epoch: 29, train_loss 2.146266222000122, train_accuracy 0.7039999961853027, val_loss 2.244328498840332, val_accuracy 0.7075999975204468\n",
            "Epoch: 30, train_loss 2.449345111846924, train_accuracy 0.699999988079071, val_loss 2.2266101837158203, val_accuracy 0.7094666957855225\n",
            "Epoch: 31, train_loss 2.422208309173584, train_accuracy 0.7179999947547913, val_loss 2.2099196910858154, val_accuracy 0.7116000056266785\n",
            "Epoch: 32, train_loss 2.288384199142456, train_accuracy 0.7200000286102295, val_loss 2.1935911178588867, val_accuracy 0.7131999731063843\n",
            "Epoch: 33, train_loss 2.1715614795684814, train_accuracy 0.699999988079071, val_loss 2.1783604621887207, val_accuracy 0.7152000069618225\n",
            "Epoch: 34, train_loss 2.547764539718628, train_accuracy 0.6919999718666077, val_loss 2.1635749340057373, val_accuracy 0.7167999744415283\n",
            "Epoch: 35, train_loss 2.5031466484069824, train_accuracy 0.699999988079071, val_loss 2.1490416526794434, val_accuracy 0.7179999947547913\n",
            "Epoch: 36, train_loss 2.031710624694824, train_accuracy 0.7279999852180481, val_loss 2.1357123851776123, val_accuracy 0.7192000150680542\n",
            "Epoch: 37, train_loss 2.091141939163208, train_accuracy 0.699999988079071, val_loss 2.1225216388702393, val_accuracy 0.7213333249092102\n",
            "Epoch: 38, train_loss 1.8429216146469116, train_accuracy 0.722000002861023, val_loss 2.109642505645752, val_accuracy 0.7218666672706604\n",
            "Epoch: 39, train_loss 1.9424277544021606, train_accuracy 0.7400000095367432, val_loss 2.0981457233428955, val_accuracy 0.7224000096321106\n",
            "Epoch: 40, train_loss 1.919010043144226, train_accuracy 0.734000027179718, val_loss 2.0859179496765137, val_accuracy 0.7235999703407288\n",
            "Epoch: 41, train_loss 2.4243674278259277, train_accuracy 0.7179999947547913, val_loss 2.0740137100219727, val_accuracy 0.7235999703407288\n",
            "Epoch: 42, train_loss 1.7072696685791016, train_accuracy 0.7720000147819519, val_loss 2.063281536102295, val_accuracy 0.7249333262443542\n",
            "Epoch: 43, train_loss 2.060267925262451, train_accuracy 0.7419999837875366, val_loss 2.0519940853118896, val_accuracy 0.7266666889190674\n",
            "Epoch: 44, train_loss 2.0951597690582275, train_accuracy 0.734000027179718, val_loss 2.041940212249756, val_accuracy 0.7265333533287048\n",
            "Epoch: 45, train_loss 2.1526548862457275, train_accuracy 0.7279999852180481, val_loss 2.031877040863037, val_accuracy 0.7274666428565979\n",
            "Epoch: 46, train_loss 1.8686896562576294, train_accuracy 0.7459999918937683, val_loss 2.0217621326446533, val_accuracy 0.7289333343505859\n",
            "Epoch: 47, train_loss 2.290006637573242, train_accuracy 0.7080000042915344, val_loss 2.012098550796509, val_accuracy 0.7296000123023987\n",
            "Epoch: 48, train_loss 2.0139853954315186, train_accuracy 0.7360000014305115, val_loss 2.0024936199188232, val_accuracy 0.7298666834831238\n",
            "Epoch: 49, train_loss 2.4895994663238525, train_accuracy 0.6800000071525574, val_loss 1.9934216737747192, val_accuracy 0.7302666902542114\n",
            "Epoch: 50, train_loss 1.8857753276824951, train_accuracy 0.7720000147819519, val_loss 1.9839798212051392, val_accuracy 0.731333315372467\n",
            "Epoch: 51, train_loss 1.962344765663147, train_accuracy 0.7419999837875366, val_loss 1.9762026071548462, val_accuracy 0.7319999933242798\n",
            "Epoch: 52, train_loss 1.6381151676177979, train_accuracy 0.7799999713897705, val_loss 1.9669444561004639, val_accuracy 0.73253333568573\n",
            "Epoch: 53, train_loss 1.6891179084777832, train_accuracy 0.7559999823570251, val_loss 1.9586706161499023, val_accuracy 0.7329333424568176\n",
            "Epoch: 54, train_loss 1.6424243450164795, train_accuracy 0.7799999713897705, val_loss 1.950573444366455, val_accuracy 0.7336000204086304\n",
            "Epoch: 55, train_loss 2.0745859146118164, train_accuracy 0.7260000109672546, val_loss 1.9424448013305664, val_accuracy 0.7338666915893555\n",
            "Epoch: 56, train_loss 1.9792917966842651, train_accuracy 0.7360000014305115, val_loss 1.934822678565979, val_accuracy 0.7343999743461609\n",
            "Epoch: 57, train_loss 1.514864206314087, train_accuracy 0.777999997138977, val_loss 1.9271012544631958, val_accuracy 0.7355999946594238\n",
            "Epoch: 58, train_loss 1.9674508571624756, train_accuracy 0.7319999933242798, val_loss 1.91940176486969, val_accuracy 0.7360000014305115\n",
            "Epoch: 59, train_loss 1.913681149482727, train_accuracy 0.7379999756813049, val_loss 1.911699652671814, val_accuracy 0.7353333234786987\n",
            "Epoch: 60, train_loss 1.7763839960098267, train_accuracy 0.7580000162124634, val_loss 1.9046615362167358, val_accuracy 0.7364000082015991\n",
            "Epoch: 61, train_loss 1.9291670322418213, train_accuracy 0.7599999904632568, val_loss 1.8974709510803223, val_accuracy 0.7376000285148621\n",
            "Epoch: 62, train_loss 1.8013256788253784, train_accuracy 0.7620000243186951, val_loss 1.890766978263855, val_accuracy 0.7379999756813049\n",
            "Epoch: 63, train_loss 1.8058940172195435, train_accuracy 0.7540000081062317, val_loss 1.883558988571167, val_accuracy 0.7386666536331177\n",
            "Epoch: 64, train_loss 1.845194697380066, train_accuracy 0.7720000147819519, val_loss 1.8771021366119385, val_accuracy 0.73826664686203\n",
            "Epoch: 65, train_loss 1.8990286588668823, train_accuracy 0.7319999933242798, val_loss 1.870238184928894, val_accuracy 0.7390666604042053\n",
            "Epoch: 66, train_loss 1.7610994577407837, train_accuracy 0.7580000162124634, val_loss 1.8639538288116455, val_accuracy 0.7387999892234802\n",
            "Epoch: 67, train_loss 2.0207438468933105, train_accuracy 0.7300000190734863, val_loss 1.85686457157135, val_accuracy 0.7400000095367432\n",
            "Epoch: 68, train_loss 2.287001848220825, train_accuracy 0.7200000286102295, val_loss 1.8506629467010498, val_accuracy 0.740933358669281\n",
            "Epoch: 69, train_loss 2.047372579574585, train_accuracy 0.7279999852180481, val_loss 1.8442761898040771, val_accuracy 0.7422666549682617\n",
            "Epoch: 70, train_loss 1.8817857503890991, train_accuracy 0.7480000257492065, val_loss 1.8384290933609009, val_accuracy 0.7429333329200745\n",
            "Epoch: 71, train_loss 1.8553783893585205, train_accuracy 0.7720000147819519, val_loss 1.8327336311340332, val_accuracy 0.7429333329200745\n",
            "Epoch: 72, train_loss 1.9833184480667114, train_accuracy 0.7540000081062317, val_loss 1.8265881538391113, val_accuracy 0.7444000244140625\n",
            "Epoch: 73, train_loss 1.9997169971466064, train_accuracy 0.7480000257492065, val_loss 1.8208656311035156, val_accuracy 0.7442666888237\n",
            "Epoch: 74, train_loss 1.7297300100326538, train_accuracy 0.7540000081062317, val_loss 1.8149516582489014, val_accuracy 0.7441333532333374\n",
            "Epoch: 75, train_loss 1.998185396194458, train_accuracy 0.7239999771118164, val_loss 1.8094600439071655, val_accuracy 0.7446666955947876\n",
            "Epoch: 76, train_loss 1.6813079118728638, train_accuracy 0.7519999742507935, val_loss 1.8035205602645874, val_accuracy 0.7446666955947876\n",
            "Epoch: 77, train_loss 1.9216699600219727, train_accuracy 0.7319999933242798, val_loss 1.7980990409851074, val_accuracy 0.7458666563034058\n",
            "Epoch: 78, train_loss 1.7052024602890015, train_accuracy 0.777999997138977, val_loss 1.7926034927368164, val_accuracy 0.7465333342552185\n",
            "Epoch: 79, train_loss 1.9373730421066284, train_accuracy 0.7639999985694885, val_loss 1.787438154220581, val_accuracy 0.7470666766166687\n",
            "Epoch: 80, train_loss 2.085594654083252, train_accuracy 0.7260000109672546, val_loss 1.7822340726852417, val_accuracy 0.7474666833877563\n",
            "Epoch: 81, train_loss 1.8740041255950928, train_accuracy 0.7419999837875366, val_loss 1.7768653631210327, val_accuracy 0.7480000257492065\n",
            "Epoch: 82, train_loss 1.6302803754806519, train_accuracy 0.7379999756813049, val_loss 1.7717548608779907, val_accuracy 0.748533308506012\n",
            "Epoch: 83, train_loss 1.9494130611419678, train_accuracy 0.7300000190734863, val_loss 1.766406536102295, val_accuracy 0.7477333545684814\n",
            "Epoch: 84, train_loss 1.6002506017684937, train_accuracy 0.7739999890327454, val_loss 1.761546015739441, val_accuracy 0.7483999729156494\n",
            "Epoch: 85, train_loss 2.101586103439331, train_accuracy 0.7179999947547913, val_loss 1.7565569877624512, val_accuracy 0.7481333613395691\n",
            "Epoch: 86, train_loss 1.9788554906845093, train_accuracy 0.7379999756813049, val_loss 1.7514927387237549, val_accuracy 0.7477333545684814\n",
            "Epoch: 87, train_loss 1.80632746219635, train_accuracy 0.7360000014305115, val_loss 1.746986746788025, val_accuracy 0.7476000189781189\n",
            "Epoch: 88, train_loss 1.9244892597198486, train_accuracy 0.75, val_loss 1.7420356273651123, val_accuracy 0.7483999729156494\n",
            "Epoch: 89, train_loss 1.5474498271942139, train_accuracy 0.7699999809265137, val_loss 1.7374478578567505, val_accuracy 0.7482666373252869\n",
            "Epoch: 90, train_loss 1.5217325687408447, train_accuracy 0.7639999985694885, val_loss 1.7325680255889893, val_accuracy 0.7481333613395691\n",
            "Epoch: 91, train_loss 1.67658269405365, train_accuracy 0.7580000162124634, val_loss 1.7277978658676147, val_accuracy 0.7487999796867371\n",
            "Epoch: 92, train_loss 1.8913460969924927, train_accuracy 0.7459999918937683, val_loss 1.723272681236267, val_accuracy 0.748533308506012\n",
            "Epoch: 93, train_loss 1.7349045276641846, train_accuracy 0.7620000243186951, val_loss 1.7187814712524414, val_accuracy 0.748533308506012\n",
            "Epoch: 94, train_loss 1.7340797185897827, train_accuracy 0.7620000243186951, val_loss 1.7144145965576172, val_accuracy 0.748533308506012\n",
            "Epoch: 95, train_loss 1.5619399547576904, train_accuracy 0.765999972820282, val_loss 1.7098604440689087, val_accuracy 0.7494666576385498\n",
            "Epoch: 96, train_loss 1.5943748950958252, train_accuracy 0.7760000228881836, val_loss 1.7055444717407227, val_accuracy 0.7491999864578247\n",
            "Epoch: 97, train_loss 1.8177882432937622, train_accuracy 0.7379999756813049, val_loss 1.7007938623428345, val_accuracy 0.7490666508674622\n",
            "Epoch: 98, train_loss 2.038533926010132, train_accuracy 0.7519999742507935, val_loss 1.6966063976287842, val_accuracy 0.7490666508674622\n",
            "Epoch: 99, train_loss 1.6645475625991821, train_accuracy 0.7480000257492065, val_loss 1.69258713722229, val_accuracy 0.7493333220481873\n",
            "Epoch: 100, train_loss 1.758525013923645, train_accuracy 0.7480000257492065, val_loss 1.6882131099700928, val_accuracy 0.7504000067710876\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for step, (batch_X, batch_Y) in enumerate(train_data.take(steps_per_epoch), 1):\n",
        "        run_optimizer(batch_X, batch_Y)\n",
        "\n",
        "    val_pred = model(valid_features)\n",
        "    val_loss = cross_entropy(val_pred, valid_labels)\n",
        "    val_acc = accuracy(val_pred, valid_labels)\n",
        "    train_pred = model(batch_X)\n",
        "    train_loss = cross_entropy(train_pred, batch_Y)\n",
        "    train_acc = accuracy(train_pred, batch_Y)\n",
        "    manager.save()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, train_loss {train_loss}, train_accuracy {train_acc}, val_loss {val_loss}, val_accuracy {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "89964352-f865-4417-afa5-26bfef13f437",
      "metadata": {
        "id": "89964352-f865-4417-afa5-26bfef13f437",
        "outputId": "eadcf656-47ea-451c-fbfe-337b63843e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_loss 1.017215609550476, Test_accuracy 0.8327999711036682\n"
          ]
        }
      ],
      "source": [
        "test_pred = model(test_features)\n",
        "test_loss = cross_entropy(test_pred, test_labels)\n",
        "test_acc = accuracy(test_pred, test_labels)\n",
        "print(f\"Test_loss {test_loss}, Test_accuracy {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbaa8601-7cee-442a-a667-c1e849ca20ce",
      "metadata": {
        "id": "cbaa8601-7cee-442a-a667-c1e849ca20ce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}