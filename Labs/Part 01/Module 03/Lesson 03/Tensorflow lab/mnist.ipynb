{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plLp_rJBvrtB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plLp_rJBvrtB",
        "outputId": "f62a3753-1530-482a-e863-bab41d4c7c98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All modules imported.\n",
            "All files downloaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 210001/210001 [00:49<00:00, 4271.56files/s]\n",
            "100%|██████████| 10001/10001 [00:01<00:00, 6174.27files/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All features and labels uncompressed.\n",
            "Tests Passed!\n",
            "Labels One-Hot Encoded\n",
            "Training features and labels randomized and split.\n",
            "Data cached in pickle file.\n"
          ]
        }
      ],
      "source": [
        "import hashlib\n",
        "import os\n",
        "import pickle\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import resample\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "print('All modules imported.')\n",
        "\n",
        "def download(url, file):\n",
        "    \"\"\"\n",
        "    Download file from <url>\n",
        "    :param url: URL to file\n",
        "    :param file: Local file path\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(file):\n",
        "        print('Downloading ' + file + '...')\n",
        "        urlretrieve(url, file)\n",
        "        print('Download Finished')\n",
        "\n",
        "# Download the training and test dataset.\n",
        "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
        "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
        "\n",
        "# Make sure the files aren't corrupted\n",
        "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
        "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
        "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
        "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
        "\n",
        "# Wait until you see that all files have been downloaded.\n",
        "print('All files downloaded.')\n",
        "\n",
        "def uncompress_features_labels(file):\n",
        "    \"\"\"\n",
        "    Uncompress features and labels from a zip file\n",
        "    :param file: The zip file to extract the data from\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with ZipFile(file) as zipf:\n",
        "        # Progress Bar\n",
        "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
        "\n",
        "        # Get features and labels from all files\n",
        "        for filename in filenames_pbar:\n",
        "            # Check if the file is a directory\n",
        "            if not filename.endswith('/'):\n",
        "                with zipf.open(filename) as image_file:\n",
        "                    image = Image.open(image_file)\n",
        "                    image.load()\n",
        "                    # Load image data as 1 dimensional array\n",
        "                    # We're using float32 to save on memory space\n",
        "                    feature = np.array(image, dtype=np.float32).flatten()\n",
        "\n",
        "                # Get the the letter from the filename.  This is the letter of the image.\n",
        "                label = os.path.split(filename)[1][0]\n",
        "\n",
        "                features.append(feature)\n",
        "                labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Get the features and labels from the zip files\n",
        "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
        "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
        "\n",
        "# Limit the amount of data to work with a docker container\n",
        "docker_size_limit = 150000\n",
        "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
        "\n",
        "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
        "is_features_normal = False\n",
        "is_labels_encod = False\n",
        "\n",
        "# Wait until you see that all features and labels have been uncompressed.\n",
        "print('All features and labels uncompressed.')\n",
        "\n",
        "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
        "def normalize_grayscale(image_data):\n",
        "    \"\"\"\n",
        "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
        "    :param image_data: The image data to be normalized\n",
        "    :return: Normalized image data\n",
        "    \"\"\"\n",
        "    # TODO: Implement Min-Max scaling for grayscale image data\n",
        "    a, b = 0.1, 0.9\n",
        "    b_a = b - a\n",
        "    min_data = 0\n",
        "    max_data = 255\n",
        "    data_diff = max_data - min_data\n",
        "    return a + ((image_data - min_data) * b_a)/(max_data - min_data)\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# Test Cases\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
        "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
        "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
        "    decimal=3)\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
        "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
        "     0.896862745098, 0.9])\n",
        "\n",
        "if not is_features_normal:\n",
        "    train_features = normalize_grayscale(train_features)\n",
        "    test_features = normalize_grayscale(test_features)\n",
        "    is_features_normal = True\n",
        "\n",
        "print('Tests Passed!')\n",
        "\n",
        "if not is_labels_encod:\n",
        "    # Turn labels into numbers and apply One-Hot Encoding\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(train_labels)\n",
        "    train_labels = encoder.transform(train_labels)\n",
        "    test_labels = encoder.transform(test_labels)\n",
        "\n",
        "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
        "    train_labels = train_labels.astype(np.float32)\n",
        "    test_labels = test_labels.astype(np.float32)\n",
        "    is_labels_encod = True\n",
        "\n",
        "print('Labels One-Hot Encoded')\n",
        "\n",
        "assert is_features_normal, 'You skipped the step to normalize the features'\n",
        "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
        "\n",
        "# Get randomized datasets for training and validation\n",
        "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    test_size=0.05,\n",
        "    random_state=832289)\n",
        "\n",
        "print('Training features and labels randomized and split.')\n",
        "\n",
        "# Save the data for easy access\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "if not os.path.isfile(pickle_file):\n",
        "    print('Saving data to pickle file...')\n",
        "    try:\n",
        "        with open('notMNIST.pickle', 'wb') as pfile:\n",
        "            pickle.dump(\n",
        "                {\n",
        "                    'train_dataset': train_features,\n",
        "                    'train_labels': train_labels,\n",
        "                    'valid_dataset': valid_features,\n",
        "                    'valid_labels': valid_labels,\n",
        "                    'test_dataset': test_features,\n",
        "                    'test_labels': test_labels,\n",
        "                },\n",
        "                pfile, pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "        print('Unable to save data to', pickle_file, ':', e)\n",
        "        raise\n",
        "\n",
        "print('Data cached in pickle file.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6d08fe36-d4fa-4292-a54c-ccc02f156920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d08fe36-d4fa-4292-a54c-ccc02f156920",
        "outputId": "f1e43ab1-03a0-4285-8ec5-4f16755baaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data and modules loaded.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Load the modules\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reload the data\n",
        "pickle_file = 'notMNIST.pickle'\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  pickle_data = pickle.load(f)\n",
        "  train_features = pickle_data['train_dataset']\n",
        "  train_labels = pickle_data['train_labels']\n",
        "  valid_features = pickle_data['valid_dataset']\n",
        "  valid_labels = pickle_data['valid_labels']\n",
        "  test_features = pickle_data['test_dataset']\n",
        "  test_labels = pickle_data['test_labels']\n",
        "  del pickle_data  # Free up memory\n",
        "\n",
        "\n",
        "print('Data and modules loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e49cb51f-2576-43b1-a76a-fa642597976d",
      "metadata": {
        "id": "e49cb51f-2576-43b1-a76a-fa642597976d"
      },
      "outputs": [],
      "source": [
        "# Data has been normalized\n",
        "total_samples = len(train_features)\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.008\n",
        "num_output = 10\n",
        "epochs = 1000\n",
        "batch_size = 256\n",
        "steps_per_epoch = int(np.ceil(total_samples / batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2dd74ff5-8247-4304-a21e-227e1dc40857",
      "metadata": {
        "id": "2dd74ff5-8247-4304-a21e-227e1dc40857"
      },
      "outputs": [],
      "source": [
        "# Cleaning Data\n",
        "#valid_labels, test_labels = np.array(valid_labels, np.int64), np.array(test_labels, np.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "val_features, test_features = np.array(valid_features, np.float32), np.array(test_features, np.float32)\n",
        "\n",
        "# Data has been normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "52a9b1dc-03ab-4628-ac52-544a0540979a",
      "metadata": {
        "id": "52a9b1dc-03ab-4628-ac52-544a0540979a"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aa753673-88d7-4601-9dff-f9ade3381a24",
      "metadata": {
        "id": "aa753673-88d7-4601-9dff-f9ade3381a24"
      },
      "outputs": [],
      "source": [
        "features = train_features.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bdc4dec7-5917-4ff5-941e-3d82566529ac",
      "metadata": {
        "id": "bdc4dec7-5917-4ff5-941e-3d82566529ac"
      },
      "outputs": [],
      "source": [
        "#W = tf.Variable(tf.random.normal([features, num_output], name=\"weights1\"))\n",
        "#B = tf.Variable(tf.zeros([num_output], name=\"bias1\"))\n",
        "\n",
        "W = {\n",
        "    \"w1\": tf.Variable(tf.random.normal([features, 512]), name=\"weight1\"),\n",
        "    #\"w2\": tf.Variable(tf.random.normal([512, 128]), name=\"weight2\"),\n",
        "    #\"w3\": tf.Variable(tf.random.normal([128, 32]), name=\"weight3\"),\n",
        "    \"out\": tf.Variable(tf.random.normal([32, num_output]), name=\"weight_out\")\n",
        "}\n",
        "\n",
        "B = {\n",
        "    \"b1\": tf.Variable(tf.zeros([512], name=\"bias1\")),\n",
        "    #\"b2\": tf.Variable(tf.zeros([128], name=\"bias2\")),\n",
        "    #\"b3\": tf.Variable(tf.zeros([32], name=\"bias3\")),\n",
        "    \"out\": tf.Variable(tf.zeros([num_output], name=\"bias_out\"))\n",
        "}\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Model\n",
        "class MyModel(tf.Module):\n",
        "    def __call__(self, X):\n",
        "      X = tf.add(tf.matmul(X, W[\"w1\"]), B[\"b1\"])\n",
        "      X = tf.nn.relu(X)\n",
        "      #X = tf.add(tf.matmul(X, W[\"w2\"]), B[\"b2\"])\n",
        "      #X = tf.nn.relu(X)\n",
        "      #X = tf.add(tf.matmul(X, W[\"w3\"]), B[\"b3\"])\n",
        "      #X = tf.nn.relu(X)\n",
        "      return tf.nn.softmax(tf.add(tf.matmul(X, W[\"out\"]), B[\"out\"]))\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    # It has been one-hot encoded before storing as pickle\n",
        "    #y_true = tf.one_hot(y_true, depth=num_output)\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), 1))\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
        "        y_true = tf.argmax(y_true, axis=1)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "def run_optimizer(X, Y):\n",
        "    with tf.GradientTape() as g:\n",
        "        logit = model(X)\n",
        "        loss = cross_entropy(logit, Y)\n",
        "\n",
        "    trainable_variables = list(W.values()) + list(B.values())\n",
        "\n",
        "    gradients = g.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return None\n",
        "\n",
        "def batch_data(X, Y, batch_size):\n",
        "    output_data = []\n",
        "    sample_size = len(X)\n",
        "    for step in range(0, sample_size, batch_size):\n",
        "        start = batch_size * step\n",
        "        end = batch_size + start\n",
        "        batch_X = X[start:end]\n",
        "        batch_Y = Y[start:end]\n",
        "        yield batch_X, batch_Y\n",
        "\n",
        "model=MyModel()\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model = model)\n",
        "checkpoint.save(\"./checkpoints/mymodel\")\n",
        "manager = tf.train.CheckpointManager(checkpoint, \"./checkpoints\", max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed6bd8f-4bdc-4110-8cec-91ba172b3b55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eed6bd8f-4bdc-4110-8cec-91ba172b3b55",
        "outputId": "90abed65-ffc7-4572-9992-f62d33c8d473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, train_loss 17.490432739257812, train_accuracy 0.15600000321865082, val_loss 17.057697296142578, val_accuracy 0.17640000581741333\n",
            "Epoch: 2, train_loss 16.371379852294922, train_accuracy 0.20999999344348907, val_loss 16.051624298095703, val_accuracy 0.22466666996479034\n",
            "Epoch: 3, train_loss 15.956913948059082, train_accuracy 0.23000000417232513, val_loss 15.429036140441895, val_accuracy 0.25519999861717224\n",
            "Epoch: 4, train_loss 14.796411514282227, train_accuracy 0.28600001335144043, val_loss 14.974984169006348, val_accuracy 0.2768000066280365\n",
            "Epoch: 5, train_loss 14.257606506347656, train_accuracy 0.31200000643730164, val_loss 14.26559829711914, val_accuracy 0.3113333284854889\n",
            "Epoch: 6, train_loss 15.045090675354004, train_accuracy 0.27399998903274536, val_loss 14.423149108886719, val_accuracy 0.303600013256073\n",
            "Epoch: 7, train_loss 15.00364875793457, train_accuracy 0.2759999930858612, val_loss 14.711017608642578, val_accuracy 0.2898666560649872\n",
            "Epoch: 8, train_loss 14.754964828491211, train_accuracy 0.2879999876022339, val_loss 14.466874122619629, val_accuracy 0.30160000920295715\n",
            "Epoch: 9, train_loss 15.127983093261719, train_accuracy 0.27000001072883606, val_loss 14.499093055725098, val_accuracy 0.30000001192092896\n",
            "Epoch: 10, train_loss 14.630624771118164, train_accuracy 0.2939999997615814, val_loss 14.382272720336914, val_accuracy 0.30586665868759155\n",
            "Epoch: 11, train_loss 15.459555625915527, train_accuracy 0.2540000081062317, val_loss 14.570289611816406, val_accuracy 0.2966666519641876\n",
            "Epoch: 12, train_loss 14.92074966430664, train_accuracy 0.2800000011920929, val_loss 14.085551261901855, val_accuracy 0.3199999928474426\n",
            "Epoch: 13, train_loss 14.008927345275879, train_accuracy 0.3240000009536743, val_loss 13.901619911193848, val_accuracy 0.329066663980484\n",
            "Epoch: 14, train_loss 13.55301570892334, train_accuracy 0.34599998593330383, val_loss 13.226019859313965, val_accuracy 0.36160001158714294\n",
            "Epoch: 15, train_loss 13.179996490478516, train_accuracy 0.36399999260902405, val_loss 12.89560604095459, val_accuracy 0.3776000142097473\n",
            "Epoch: 16, train_loss 12.931316375732422, train_accuracy 0.37599998712539673, val_loss 13.040753364562988, val_accuracy 0.37040001153945923\n",
            "Epoch: 17, train_loss 12.55829906463623, train_accuracy 0.39399999380111694, val_loss 12.530983924865723, val_accuracy 0.39480000734329224\n",
            "Epoch: 18, train_loss 12.009026527404785, train_accuracy 0.4180000126361847, val_loss 11.773491859436035, val_accuracy 0.4317333400249481\n",
            "Epoch: 19, train_loss 12.185279846191406, train_accuracy 0.41200000047683716, val_loss 11.465958595275879, val_accuracy 0.446399986743927\n",
            "Epoch: 20, train_loss 11.770814895629883, train_accuracy 0.4320000112056732, val_loss 11.351654052734375, val_accuracy 0.45213332772254944\n",
            "Epoch: 21, train_loss 10.40307903289795, train_accuracy 0.49799999594688416, val_loss 10.842057228088379, val_accuracy 0.47653332352638245\n",
            "Epoch: 22, train_loss 10.361632347106934, train_accuracy 0.5, val_loss 10.084927558898926, val_accuracy 0.5130666494369507\n",
            "Epoch: 23, train_loss 9.366914749145508, train_accuracy 0.5479999780654907, val_loss 9.961854934692383, val_accuracy 0.5190666913986206\n",
            "Epoch: 24, train_loss 9.491255760192871, train_accuracy 0.5419999957084656, val_loss 9.831897735595703, val_accuracy 0.5252000093460083\n",
            "Epoch: 25, train_loss 9.449807167053223, train_accuracy 0.5440000295639038, val_loss 9.692956924438477, val_accuracy 0.5322666764259338\n",
            "Epoch: 26, train_loss 9.491254806518555, train_accuracy 0.5419999957084656, val_loss 9.580626487731934, val_accuracy 0.5371999740600586\n",
            "Epoch: 27, train_loss 9.781380653381348, train_accuracy 0.527999997138977, val_loss 9.757065773010254, val_accuracy 0.5289333462715149\n",
            "Epoch: 28, train_loss 9.98861312866211, train_accuracy 0.5180000066757202, val_loss 9.64769458770752, val_accuracy 0.5342666506767273\n",
            "Epoch: 29, train_loss 9.615594863891602, train_accuracy 0.5360000133514404, val_loss 9.506546020507812, val_accuracy 0.5410666465759277\n",
            "Epoch: 30, train_loss 9.449808120727539, train_accuracy 0.5440000295639038, val_loss 9.463667869567871, val_accuracy 0.5432000160217285\n",
            "Epoch: 31, train_loss 9.6570405960083, train_accuracy 0.5339999794960022, val_loss 9.32724380493164, val_accuracy 0.5497333407402039\n",
            "Epoch: 32, train_loss 8.952448844909668, train_accuracy 0.5680000185966492, val_loss 9.389019966125488, val_accuracy 0.5469333529472351\n",
            "Epoch: 33, train_loss 8.917070388793945, train_accuracy 0.5680000185966492, val_loss 9.42585277557373, val_accuracy 0.5447999835014343\n",
            "Epoch: 34, train_loss 8.869556427001953, train_accuracy 0.5720000267028809, val_loss 9.370856285095215, val_accuracy 0.5475999712944031\n",
            "Epoch: 35, train_loss 8.786664009094238, train_accuracy 0.5759999752044678, val_loss 9.276745796203613, val_accuracy 0.5522666573524475\n",
            "Epoch: 36, train_loss 9.213804244995117, train_accuracy 0.5540000200271606, val_loss 9.332155227661133, val_accuracy 0.5491999983787537\n",
            "Epoch: 37, train_loss 8.993896484375, train_accuracy 0.5659999847412109, val_loss 9.480340957641602, val_accuracy 0.5421333312988281\n",
            "Epoch: 38, train_loss 8.962669372558594, train_accuracy 0.5659999847412109, val_loss 9.616787910461426, val_accuracy 0.5356000065803528\n",
            "Epoch: 39, train_loss 9.366914749145508, train_accuracy 0.5479999780654907, val_loss 9.342183113098145, val_accuracy 0.5490666627883911\n",
            "Epoch: 40, train_loss 9.408361434936523, train_accuracy 0.5460000038146973, val_loss 9.418194770812988, val_accuracy 0.5450666546821594\n",
            "Epoch: 41, train_loss 9.076789855957031, train_accuracy 0.5619999766349792, val_loss 9.164813995361328, val_accuracy 0.5574666857719421\n",
            "Epoch: 42, train_loss 8.86955738067627, train_accuracy 0.5720000267028809, val_loss 9.140035629272461, val_accuracy 0.5587999820709229\n",
            "Epoch: 43, train_loss 8.247859001159668, train_accuracy 0.6019999980926514, val_loss 9.164067268371582, val_accuracy 0.5577333569526672\n",
            "Epoch: 44, train_loss 9.366914749145508, train_accuracy 0.5479999780654907, val_loss 9.188080787658691, val_accuracy 0.5564000010490417\n",
            "Epoch: 45, train_loss 9.242575645446777, train_accuracy 0.5540000200271606, val_loss 9.110832214355469, val_accuracy 0.5601333379745483\n",
            "Epoch: 46, train_loss 9.615594863891602, train_accuracy 0.5360000133514404, val_loss 9.31277084350586, val_accuracy 0.5504000186920166\n",
            "Epoch: 47, train_loss 8.41364574432373, train_accuracy 0.593999981880188, val_loss 9.063446998596191, val_accuracy 0.5623999834060669\n",
            "Epoch: 48, train_loss 8.745216369628906, train_accuracy 0.578000009059906, val_loss 9.171634674072266, val_accuracy 0.557200014591217\n",
            "Epoch: 49, train_loss 9.284022331237793, train_accuracy 0.5519999861717224, val_loss 9.105194091796875, val_accuracy 0.5604000091552734\n",
            "Epoch: 50, train_loss 9.739933967590332, train_accuracy 0.5299999713897705, val_loss 9.045392990112305, val_accuracy 0.5631999969482422\n",
            "Epoch: 51, train_loss 8.745216369628906, train_accuracy 0.578000009059906, val_loss 9.1242094039917, val_accuracy 0.559333324432373\n",
            "Epoch: 52, train_loss 9.201128959655762, train_accuracy 0.5559999942779541, val_loss 9.0283784866333, val_accuracy 0.5642666816711426\n",
            "Epoch: 53, train_loss 8.45509147644043, train_accuracy 0.5920000076293945, val_loss 9.306124687194824, val_accuracy 0.5509333610534668\n",
            "Epoch: 54, train_loss 8.57943058013916, train_accuracy 0.5860000252723694, val_loss 9.026086807250977, val_accuracy 0.5642666816711426\n",
            "Epoch: 55, train_loss 9.739933967590332, train_accuracy 0.5299999713897705, val_loss 8.985605239868164, val_accuracy 0.5663999915122986\n",
            "Epoch: 56, train_loss 8.123519897460938, train_accuracy 0.6079999804496765, val_loss 8.998709678649902, val_accuracy 0.5654666423797607\n",
            "Epoch: 57, train_loss 9.491255760192871, train_accuracy 0.5419999957084656, val_loss 9.055561065673828, val_accuracy 0.562666654586792\n",
            "Epoch: 58, train_loss 8.869556427001953, train_accuracy 0.5720000267028809, val_loss 9.049959182739258, val_accuracy 0.5630666613578796\n",
            "Epoch: 59, train_loss 8.828110694885254, train_accuracy 0.5740000009536743, val_loss 9.048532485961914, val_accuracy 0.5631999969482422\n",
            "Epoch: 60, train_loss 9.076788902282715, train_accuracy 0.5619999766349792, val_loss 9.059111595153809, val_accuracy 0.5625333189964294\n",
            "Epoch: 61, train_loss 8.745217323303223, train_accuracy 0.578000009059906, val_loss 8.966263771057129, val_accuracy 0.5673333406448364\n",
            "Epoch: 62, train_loss 8.952449798583984, train_accuracy 0.5680000185966492, val_loss 9.405354499816895, val_accuracy 0.5461333394050598\n",
            "Epoch: 63, train_loss 8.786664009094238, train_accuracy 0.5759999752044678, val_loss 9.33181381225586, val_accuracy 0.5493333339691162\n",
            "Epoch: 64, train_loss 9.366914749145508, train_accuracy 0.5479999780654907, val_loss 8.992582321166992, val_accuracy 0.5659999847412109\n",
            "Epoch: 65, train_loss 8.740350723266602, train_accuracy 0.578000009059906, val_loss 9.029111862182617, val_accuracy 0.56413334608078\n",
            "Epoch: 66, train_loss 8.662323951721191, train_accuracy 0.5820000171661377, val_loss 8.943705558776855, val_accuracy 0.5682666897773743\n",
            "Epoch: 67, train_loss 9.57414722442627, train_accuracy 0.5379999876022339, val_loss 9.0016508102417, val_accuracy 0.5655999779701233\n",
            "Epoch: 68, train_loss 9.201128959655762, train_accuracy 0.5559999942779541, val_loss 8.933013916015625, val_accuracy 0.5689333081245422\n",
            "Epoch: 69, train_loss 7.709053993225098, train_accuracy 0.628000020980835, val_loss 8.942474365234375, val_accuracy 0.5680000185966492\n",
            "Epoch: 70, train_loss 8.745217323303223, train_accuracy 0.578000009059906, val_loss 9.020737648010254, val_accuracy 0.5644000172615051\n",
            "Epoch: 71, train_loss 8.579431533813477, train_accuracy 0.5860000252723694, val_loss 8.949997901916504, val_accuracy 0.5677333474159241\n",
            "Epoch: 72, train_loss 8.911004066467285, train_accuracy 0.5699999928474426, val_loss 8.840376853942871, val_accuracy 0.573199987411499\n",
            "Epoch: 73, train_loss 9.905719757080078, train_accuracy 0.5220000147819519, val_loss 8.841924667358398, val_accuracy 0.5733333230018616\n",
            "Epoch: 74, train_loss 9.574148178100586, train_accuracy 0.5379999876022339, val_loss 8.961478233337402, val_accuracy 0.5673333406448364\n",
            "Epoch: 75, train_loss 8.206412315368652, train_accuracy 0.6039999723434448, val_loss 9.009978294372559, val_accuracy 0.5651999711990356\n",
            "Epoch: 76, train_loss 8.579431533813477, train_accuracy 0.5860000252723694, val_loss 8.933309555053711, val_accuracy 0.5687999725341797\n",
            "Epoch: 77, train_loss 9.98861312866211, train_accuracy 0.5180000066757202, val_loss 8.847519874572754, val_accuracy 0.5729333162307739\n",
            "Epoch: 78, train_loss 9.201128959655762, train_accuracy 0.5559999942779541, val_loss 9.10287857055664, val_accuracy 0.5604000091552734\n",
            "Epoch: 79, train_loss 10.07150650024414, train_accuracy 0.5139999985694885, val_loss 8.905978202819824, val_accuracy 0.5699999928474426\n",
            "Epoch: 80, train_loss 8.703770637512207, train_accuracy 0.5799999833106995, val_loss 8.89503002166748, val_accuracy 0.5705333352088928\n",
            "Epoch: 81, train_loss 9.98861312866211, train_accuracy 0.5180000066757202, val_loss 8.97175121307373, val_accuracy 0.5666666626930237\n",
            "Epoch: 82, train_loss 8.828110694885254, train_accuracy 0.5740000009536743, val_loss 8.997299194335938, val_accuracy 0.5655999779701233\n",
            "Epoch: 83, train_loss 8.869556427001953, train_accuracy 0.5720000267028809, val_loss 8.859557151794434, val_accuracy 0.5722666382789612\n",
            "Epoch: 84, train_loss 8.16566276550293, train_accuracy 0.6060000061988831, val_loss 8.850346565246582, val_accuracy 0.5727999806404114\n",
            "Epoch: 85, train_loss 8.620878219604492, train_accuracy 0.5839999914169312, val_loss 8.900279998779297, val_accuracy 0.5703999996185303\n",
            "Epoch: 86, train_loss 8.276594161987305, train_accuracy 0.6000000238418579, val_loss 8.801267623901367, val_accuracy 0.5750666856765747\n",
            "Epoch: 87, train_loss 8.662323951721191, train_accuracy 0.5820000171661377, val_loss 8.80278205871582, val_accuracy 0.5749333500862122\n",
            "Epoch: 88, train_loss 9.864273071289062, train_accuracy 0.5239999890327454, val_loss 9.226853370666504, val_accuracy 0.5544000267982483\n",
            "Epoch: 89, train_loss 8.57943058013916, train_accuracy 0.5860000252723694, val_loss 8.992096900939941, val_accuracy 0.5658666491508484\n",
            "Epoch: 90, train_loss 8.952449798583984, train_accuracy 0.5680000185966492, val_loss 8.752894401550293, val_accuracy 0.5773333311080933\n",
            "Epoch: 91, train_loss 7.584714889526367, train_accuracy 0.6340000033378601, val_loss 8.50672721862793, val_accuracy 0.5892000198364258\n",
            "Epoch: 92, train_loss 7.916286468505859, train_accuracy 0.6179999709129333, val_loss 8.053322792053223, val_accuracy 0.6111999750137329\n",
            "Epoch: 93, train_loss 8.786663055419922, train_accuracy 0.5759999752044678, val_loss 8.16930103302002, val_accuracy 0.6055999994277954\n",
            "Epoch: 94, train_loss 8.082074165344238, train_accuracy 0.6100000143051147, val_loss 7.706607341766357, val_accuracy 0.628000020980835\n",
            "Epoch: 95, train_loss 8.41364574432373, train_accuracy 0.593999981880188, val_loss 7.974024295806885, val_accuracy 0.614799976348877\n",
            "Epoch: 96, train_loss 8.455090522766113, train_accuracy 0.5920000076293945, val_loss 7.827561378479004, val_accuracy 0.621999979019165\n",
            "Epoch: 97, train_loss 7.12880277633667, train_accuracy 0.656000018119812, val_loss 7.647585868835449, val_accuracy 0.6305333375930786\n",
            "Epoch: 98, train_loss 7.833394527435303, train_accuracy 0.621999979019165, val_loss 7.5441694259643555, val_accuracy 0.6354666948318481\n",
            "Epoch: 99, train_loss 7.377481937408447, train_accuracy 0.6439999938011169, val_loss 7.394087791442871, val_accuracy 0.6431999802589417\n",
            "Epoch: 100, train_loss 7.377481937408447, train_accuracy 0.6439999938011169, val_loss 7.4625067710876465, val_accuracy 0.6397333145141602\n",
            "Epoch: 101, train_loss 7.004462718963623, train_accuracy 0.6620000004768372, val_loss 7.447300434112549, val_accuracy 0.6402666568756104\n",
            "Epoch: 102, train_loss 7.12880277633667, train_accuracy 0.656000018119812, val_loss 7.319036483764648, val_accuracy 0.6466666460037231\n",
            "Epoch: 103, train_loss 7.293886661529541, train_accuracy 0.6480000019073486, val_loss 7.350875377655029, val_accuracy 0.6449333429336548\n",
            "Epoch: 104, train_loss 7.501821994781494, train_accuracy 0.6380000114440918, val_loss 7.45977783203125, val_accuracy 0.6399999856948853\n",
            "Epoch: 105, train_loss 7.501821994781494, train_accuracy 0.6380000114440918, val_loss 7.256284713745117, val_accuracy 0.6496000289916992\n",
            "Epoch: 106, train_loss 6.838677406311035, train_accuracy 0.6700000166893005, val_loss 7.341775417327881, val_accuracy 0.6456000208854675\n",
            "Epoch: 107, train_loss 6.963016986846924, train_accuracy 0.6639999747276306, val_loss 7.203588008880615, val_accuracy 0.6518666744232178\n",
            "Epoch: 108, train_loss 7.66760778427124, train_accuracy 0.6299999952316284, val_loss 7.308631420135498, val_accuracy 0.6470666527748108\n",
            "Epoch: 109, train_loss 8.289305686950684, train_accuracy 0.6000000238418579, val_loss 7.38011360168457, val_accuracy 0.6437333226203918\n",
            "Epoch: 110, train_loss 6.755784034729004, train_accuracy 0.6740000247955322, val_loss 7.22993278503418, val_accuracy 0.6510666608810425\n",
            "Epoch: 111, train_loss 6.963016510009766, train_accuracy 0.6639999747276306, val_loss 7.249516010284424, val_accuracy 0.649733304977417\n",
            "Epoch: 112, train_loss 7.294589042663574, train_accuracy 0.6480000019073486, val_loss 7.281733989715576, val_accuracy 0.6485333442687988\n",
            "Epoch: 113, train_loss 7.418928623199463, train_accuracy 0.6420000195503235, val_loss 7.210177898406982, val_accuracy 0.6520000100135803\n",
            "Epoch: 114, train_loss 6.797230243682861, train_accuracy 0.671999990940094, val_loss 7.372363567352295, val_accuracy 0.6441333293914795\n",
            "Epoch: 115, train_loss 7.750500679016113, train_accuracy 0.6259999871253967, val_loss 7.146024703979492, val_accuracy 0.6550666689872742\n",
            "Epoch: 116, train_loss 7.501821517944336, train_accuracy 0.6380000114440918, val_loss 7.151187896728516, val_accuracy 0.6547999978065491\n",
            "Epoch: 117, train_loss 7.1702494621276855, train_accuracy 0.6539999842643738, val_loss 7.167238235473633, val_accuracy 0.6537333130836487\n",
            "Epoch: 118, train_loss 7.709053993225098, train_accuracy 0.628000020980835, val_loss 8.173534393310547, val_accuracy 0.6050666570663452\n",
            "Epoch: 119, train_loss 6.921570301055908, train_accuracy 0.6660000085830688, val_loss 7.1570725440979, val_accuracy 0.6543999910354614\n",
            "Epoch: 120, train_loss 7.253142356872559, train_accuracy 0.6499999761581421, val_loss 7.125772476196289, val_accuracy 0.6558666825294495\n",
            "Epoch: 121, train_loss 7.874840259552002, train_accuracy 0.6200000047683716, val_loss 7.101803302764893, val_accuracy 0.6571999788284302\n",
            "Epoch: 122, train_loss 6.465658187866211, train_accuracy 0.6880000233650208, val_loss 7.146059989929199, val_accuracy 0.6550666689872742\n",
            "Epoch: 123, train_loss 6.7972307205200195, train_accuracy 0.671999990940094, val_loss 7.075510501861572, val_accuracy 0.6583999991416931\n",
            "Epoch: 124, train_loss 7.377481937408447, train_accuracy 0.6439999938011169, val_loss 7.284320831298828, val_accuracy 0.6481333374977112\n",
            "Epoch: 125, train_loss 7.12880277633667, train_accuracy 0.656000018119812, val_loss 7.12880277633667, val_accuracy 0.656000018119812\n",
            "Epoch: 126, train_loss 7.253142356872559, train_accuracy 0.6499999761581421, val_loss 7.140985488891602, val_accuracy 0.6552000045776367\n",
            "Epoch: 127, train_loss 7.12880277633667, train_accuracy 0.656000018119812, val_loss 7.1779866218566895, val_accuracy 0.6534666419029236\n",
            "Epoch: 128, train_loss 7.377481460571289, train_accuracy 0.6439999938011169, val_loss 7.1187214851379395, val_accuracy 0.6561333537101746\n",
            "Epoch: 129, train_loss 7.1702494621276855, train_accuracy 0.6539999842643738, val_loss 7.1381659507751465, val_accuracy 0.6554666757583618\n",
            "Epoch: 130, train_loss 6.299872398376465, train_accuracy 0.6959999799728394, val_loss 7.018697261810303, val_accuracy 0.6610666513442993\n",
            "Epoch: 131, train_loss 6.714337348937988, train_accuracy 0.6759999990463257, val_loss 7.070240497589111, val_accuracy 0.6588000059127808\n",
            "Epoch: 132, train_loss 6.880124092102051, train_accuracy 0.6679999828338623, val_loss 7.047954559326172, val_accuracy 0.6598666906356812\n",
            "Epoch: 133, train_loss 7.170248985290527, train_accuracy 0.6539999842643738, val_loss 7.063591957092285, val_accuracy 0.6590666770935059\n",
            "Epoch: 134, train_loss 7.045909881591797, train_accuracy 0.6600000262260437, val_loss 7.084791660308838, val_accuracy 0.6578666567802429\n",
            "Epoch: 135, train_loss 7.004463195800781, train_accuracy 0.6620000004768372, val_loss 7.196217060089111, val_accuracy 0.652400016784668\n",
            "Epoch: 136, train_loss 7.66760778427124, train_accuracy 0.6299999952316284, val_loss 7.073680877685547, val_accuracy 0.6585333347320557\n",
            "Epoch: 137, train_loss 7.12880277633667, train_accuracy 0.656000018119812, val_loss 7.293514728546143, val_accuracy 0.6478666663169861\n",
            "Epoch: 138, train_loss 6.880123615264893, train_accuracy 0.6679999828338623, val_loss 7.153619766235352, val_accuracy 0.6546666622161865\n",
            "Epoch: 139, train_loss 7.087356090545654, train_accuracy 0.6579999923706055, val_loss 7.061394214630127, val_accuracy 0.6592000126838684\n",
            "Epoch: 140, train_loss 7.377481460571289, train_accuracy 0.6439999938011169, val_loss 7.013169288635254, val_accuracy 0.661466658115387\n",
            "Epoch: 141, train_loss 6.880123615264893, train_accuracy 0.6679999828338623, val_loss 7.075342655181885, val_accuracy 0.6585333347320557\n",
            "Epoch: 142, train_loss 6.921570301055908, train_accuracy 0.6660000085830688, val_loss 7.2001495361328125, val_accuracy 0.652400016784668\n",
            "Epoch: 143, train_loss 6.589998722076416, train_accuracy 0.6819999814033508, val_loss 6.966164588928223, val_accuracy 0.6636000275611877\n",
            "Epoch: 144, train_loss 7.377481937408447, train_accuracy 0.6439999938011169, val_loss 7.078597545623779, val_accuracy 0.6582666635513306\n",
            "Epoch: 145, train_loss 7.501821994781494, train_accuracy 0.6380000114440918, val_loss 7.105565071105957, val_accuracy 0.6570666432380676\n",
            "Epoch: 146, train_loss 6.880123615264893, train_accuracy 0.6679999828338623, val_loss 7.037250518798828, val_accuracy 0.660266637802124\n",
            "Epoch: 147, train_loss 6.507104873657227, train_accuracy 0.6859999895095825, val_loss 7.10496711730957, val_accuracy 0.6570666432380676\n",
            "Epoch: 148, train_loss 6.672891139984131, train_accuracy 0.6779999732971191, val_loss 7.034502029418945, val_accuracy 0.6603999733924866\n",
            "Epoch: 149, train_loss 6.4242119789123535, train_accuracy 0.6899999976158142, val_loss 7.0293498039245605, val_accuracy 0.6607999801635742\n",
            "Epoch: 150, train_loss 7.12880277633667, train_accuracy 0.656000018119812, val_loss 7.072568893432617, val_accuracy 0.6585333347320557\n",
            "Epoch: 151, train_loss 6.424212455749512, train_accuracy 0.6899999976158142, val_loss 6.982604503631592, val_accuracy 0.662933349609375\n",
            "Epoch: 152, train_loss 6.838676929473877, train_accuracy 0.6700000166893005, val_loss 7.083615303039551, val_accuracy 0.6579999923706055\n",
            "Epoch: 153, train_loss 7.460374355316162, train_accuracy 0.6399999856948853, val_loss 7.136695861816406, val_accuracy 0.6554666757583618\n",
            "Epoch: 154, train_loss 7.709053993225098, train_accuracy 0.628000020980835, val_loss 7.170248508453369, val_accuracy 0.6538666486740112\n",
            "Epoch: 155, train_loss 6.382765293121338, train_accuracy 0.6919999718666077, val_loss 6.992104530334473, val_accuracy 0.6625333428382874\n",
            "Epoch: 156, train_loss 6.880124092102051, train_accuracy 0.6679999828338623, val_loss 7.002073287963867, val_accuracy 0.6620000004768372\n",
            "Epoch: 157, train_loss 6.838676929473877, train_accuracy 0.6700000166893005, val_loss 7.120111465454102, val_accuracy 0.6562666893005371\n",
            "Epoch: 158, train_loss 7.501821994781494, train_accuracy 0.6380000114440918, val_loss 7.029740333557129, val_accuracy 0.6606666445732117\n",
            "Epoch: 159, train_loss 7.294589042663574, train_accuracy 0.6480000019073486, val_loss 7.088831901550293, val_accuracy 0.6578666567802429\n",
            "Epoch: 160, train_loss 6.963016986846924, train_accuracy 0.6639999747276306, val_loss 7.127745628356934, val_accuracy 0.656000018119812\n",
            "Epoch: 161, train_loss 7.1702494621276855, train_accuracy 0.6539999842643738, val_loss 6.9951043128967285, val_accuracy 0.6622666716575623\n",
            "Epoch: 162, train_loss 7.253142356872559, train_accuracy 0.6499999761581421, val_loss 7.1112961769104, val_accuracy 0.6565333604812622\n",
            "Epoch: 163, train_loss 6.507104396820068, train_accuracy 0.6859999895095825, val_loss 6.986457824707031, val_accuracy 0.6628000140190125\n",
            "Epoch: 164, train_loss 6.921570301055908, train_accuracy 0.6660000085830688, val_loss 6.939182281494141, val_accuracy 0.665066659450531\n",
            "Epoch: 165, train_loss 6.797230243682861, train_accuracy 0.671999990940094, val_loss 6.880124092102051, val_accuracy 0.6679999828338623\n",
            "Epoch: 166, train_loss 7.336035251617432, train_accuracy 0.6460000276565552, val_loss 7.018712520599365, val_accuracy 0.6611999869346619\n",
            "Epoch: 167, train_loss 7.501821994781494, train_accuracy 0.6380000114440918, val_loss 6.954209327697754, val_accuracy 0.6641333103179932\n",
            "Epoch: 168, train_loss 6.507104873657227, train_accuracy 0.6859999895095825, val_loss 6.961184024810791, val_accuracy 0.6638666391372681\n",
            "Epoch: 169, train_loss 6.797230243682861, train_accuracy 0.671999990940094, val_loss 6.967288017272949, val_accuracy 0.6634666919708252\n",
            "Epoch: 170, train_loss 6.963016510009766, train_accuracy 0.6639999747276306, val_loss 6.954359531402588, val_accuracy 0.6639999747276306\n",
            "Epoch: 171, train_loss 6.216979026794434, train_accuracy 0.699999988079071, val_loss 6.9181599617004395, val_accuracy 0.6660000085830688\n",
            "Epoch: 172, train_loss 7.211695671081543, train_accuracy 0.6520000100135803, val_loss 7.111995220184326, val_accuracy 0.6566666960716248\n",
            "Epoch: 173, train_loss 6.382765293121338, train_accuracy 0.6919999718666077, val_loss 6.985852241516113, val_accuracy 0.6626666784286499\n",
            "Epoch: 174, train_loss 6.507104396820068, train_accuracy 0.6859999895095825, val_loss 6.979188442230225, val_accuracy 0.6630666851997375\n",
            "Epoch: 175, train_loss 6.797230243682861, train_accuracy 0.671999990940094, val_loss 7.007936000823975, val_accuracy 0.6617333292961121\n",
            "Epoch: 176, train_loss 6.548551559448242, train_accuracy 0.6840000152587891, val_loss 6.971640110015869, val_accuracy 0.6634666919708252\n",
            "Epoch: 177, train_loss 7.336035251617432, train_accuracy 0.6460000276565552, val_loss 7.082647323608398, val_accuracy 0.658133327960968\n",
            "Epoch: 178, train_loss 7.1702494621276855, train_accuracy 0.6539999842643738, val_loss 6.969488620758057, val_accuracy 0.6634666919708252\n",
            "Epoch: 179, train_loss 7.045909881591797, train_accuracy 0.6600000262260437, val_loss 6.949248313903809, val_accuracy 0.6646666526794434\n",
            "Epoch: 180, train_loss 6.672890663146973, train_accuracy 0.6779999732971191, val_loss 6.880177974700928, val_accuracy 0.6677333116531372\n",
            "Epoch: 181, train_loss 7.46037483215332, train_accuracy 0.6399999856948853, val_loss 6.976832866668701, val_accuracy 0.6633333563804626\n",
            "Epoch: 182, train_loss 7.211695671081543, train_accuracy 0.6520000100135803, val_loss 6.909778594970703, val_accuracy 0.6664000153541565\n",
            "Epoch: 183, train_loss 7.1702494621276855, train_accuracy 0.6539999842643738, val_loss 6.833427429199219, val_accuracy 0.6701333522796631\n",
            "Epoch: 184, train_loss 7.377481937408447, train_accuracy 0.6439999938011169, val_loss 6.9430718421936035, val_accuracy 0.6647999882698059\n",
            "Epoch: 185, train_loss 7.045909881591797, train_accuracy 0.6600000262260437, val_loss 7.026235580444336, val_accuracy 0.6609333157539368\n",
            "Epoch: 186, train_loss 7.791947364807129, train_accuracy 0.6240000128746033, val_loss 6.946095943450928, val_accuracy 0.6645333170890808\n",
            "Epoch: 187, train_loss 8.04062557220459, train_accuracy 0.6119999885559082, val_loss 6.992311000823975, val_accuracy 0.6624000072479248\n",
            "Epoch: 188, train_loss 7.1702494621276855, train_accuracy 0.6539999842643738, val_loss 7.041055679321289, val_accuracy 0.6601333618164062\n",
            "Epoch: 189, train_loss 7.294589519500732, train_accuracy 0.6480000019073486, val_loss 6.954285621643066, val_accuracy 0.6643999814987183\n",
            "Epoch: 190, train_loss 7.163326263427734, train_accuracy 0.6539999842643738, val_loss 7.338798999786377, val_accuracy 0.6458666920661926\n",
            "Epoch: 191, train_loss 7.39116907119751, train_accuracy 0.6420000195503235, val_loss 7.026684761047363, val_accuracy 0.6606666445732117\n",
            "Epoch: 192, train_loss 6.548551082611084, train_accuracy 0.6840000152587891, val_loss 6.903810977935791, val_accuracy 0.6666666865348816\n",
            "Epoch: 193, train_loss 6.631444931030273, train_accuracy 0.6800000071525574, val_loss 6.9770965576171875, val_accuracy 0.6630666851997375\n",
            "Epoch: 194, train_loss 7.667607307434082, train_accuracy 0.6299999952316284, val_loss 6.9694390296936035, val_accuracy 0.6634666919708252\n",
            "Epoch: 195, train_loss 6.755784034729004, train_accuracy 0.6740000247955322, val_loss 6.934122085571289, val_accuracy 0.665066659450531\n",
            "Epoch: 196, train_loss 6.465658187866211, train_accuracy 0.6880000233650208, val_loss 7.010779857635498, val_accuracy 0.661466658115387\n",
            "Epoch: 197, train_loss 7.957733631134033, train_accuracy 0.6159999966621399, val_loss 7.631209850311279, val_accuracy 0.631600022315979\n",
            "Epoch: 198, train_loss 6.507104396820068, train_accuracy 0.6859999895095825, val_loss 6.917793273925781, val_accuracy 0.6661333441734314\n",
            "Epoch: 199, train_loss 7.501821517944336, train_accuracy 0.6380000114440918, val_loss 7.154786109924316, val_accuracy 0.6541333198547363\n",
            "Epoch: 200, train_loss 6.838676929473877, train_accuracy 0.6700000166893005, val_loss 6.918641567230225, val_accuracy 0.6661333441734314\n",
            "Epoch: 201, train_loss 6.507104873657227, train_accuracy 0.6859999895095825, val_loss 6.940222263336182, val_accuracy 0.665066659450531\n",
            "Epoch: 202, train_loss 6.921570301055908, train_accuracy 0.6660000085830688, val_loss 6.862846374511719, val_accuracy 0.668666660785675\n",
            "Epoch: 203, train_loss 7.336035251617432, train_accuracy 0.6460000276565552, val_loss 7.0100016593933105, val_accuracy 0.6617333292961121\n",
            "Epoch: 204, train_loss 7.874840259552002, train_accuracy 0.6200000047683716, val_loss 6.9077558517456055, val_accuracy 0.6666666865348816\n",
            "Epoch: 205, train_loss 6.175532817840576, train_accuracy 0.7020000219345093, val_loss 6.884460926055908, val_accuracy 0.6675999760627747\n",
            "Epoch: 206, train_loss 7.584714889526367, train_accuracy 0.6340000033378601, val_loss 7.341641902923584, val_accuracy 0.645466685295105\n",
            "Epoch: 207, train_loss 7.12880277633667, train_accuracy 0.656000018119812, val_loss 6.958752155303955, val_accuracy 0.6637333035469055\n",
            "Epoch: 208, train_loss 7.087356090545654, train_accuracy 0.6579999923706055, val_loss 6.863587379455566, val_accuracy 0.6687999963760376\n",
            "Epoch: 209, train_loss 6.755784034729004, train_accuracy 0.6740000247955322, val_loss 6.867511749267578, val_accuracy 0.66839998960495\n",
            "Epoch: 210, train_loss 6.589998245239258, train_accuracy 0.6819999814033508, val_loss 6.841440200805664, val_accuracy 0.669866681098938\n",
            "Epoch: 211, train_loss 6.838676929473877, train_accuracy 0.6700000166893005, val_loss 6.9591965675354, val_accuracy 0.6641333103179932\n",
            "Epoch: 212, train_loss 6.341319561004639, train_accuracy 0.6940000057220459, val_loss 6.798036098480225, val_accuracy 0.6718666553497314\n",
            "Epoch: 213, train_loss 6.5899977684021, train_accuracy 0.6819999814033508, val_loss 6.8348307609558105, val_accuracy 0.6700000166893005\n",
            "Epoch: 214, train_loss 7.1702494621276855, train_accuracy 0.6539999842643738, val_loss 6.902573585510254, val_accuracy 0.6668000221252441\n",
            "Epoch: 215, train_loss 6.3413190841674805, train_accuracy 0.6940000057220459, val_loss 6.897646427154541, val_accuracy 0.6669333577156067\n",
            "Epoch: 216, train_loss 6.631444454193115, train_accuracy 0.6800000071525574, val_loss 6.887739181518555, val_accuracy 0.6675999760627747\n",
            "Epoch: 217, train_loss 6.921570301055908, train_accuracy 0.6660000085830688, val_loss 6.824077129364014, val_accuracy 0.6705333590507507\n",
            "Epoch: 218, train_loss 6.921570301055908, train_accuracy 0.6660000085830688, val_loss 6.910156726837158, val_accuracy 0.666266679763794\n",
            "Epoch: 219, train_loss 7.294589042663574, train_accuracy 0.6480000019073486, val_loss 6.896702766418457, val_accuracy 0.6672000288963318\n",
            "Epoch: 220, train_loss 6.755786418914795, train_accuracy 0.6740000247955322, val_loss 6.8714776039123535, val_accuracy 0.66839998960495\n",
            "Epoch: 221, train_loss 6.963016986846924, train_accuracy 0.6639999747276306, val_loss 6.839719772338867, val_accuracy 0.6697333455085754\n",
            "Epoch: 222, train_loss 6.589998245239258, train_accuracy 0.6819999814033508, val_loss 6.984142780303955, val_accuracy 0.6626666784286499\n",
            "Epoch: 223, train_loss 7.045909881591797, train_accuracy 0.6600000262260437, val_loss 6.916253566741943, val_accuracy 0.6661333441734314\n",
            "Epoch: 224, train_loss 6.465658187866211, train_accuracy 0.6880000233650208, val_loss 6.891653060913086, val_accuracy 0.6673333048820496\n",
            "Epoch: 225, train_loss 6.797230243682861, train_accuracy 0.671999990940094, val_loss 6.9601640701293945, val_accuracy 0.6641333103179932\n",
            "Epoch: 226, train_loss 7.501821517944336, train_accuracy 0.6380000114440918, val_loss 6.897101879119873, val_accuracy 0.6670666933059692\n",
            "Epoch: 227, train_loss 6.631444931030273, train_accuracy 0.6800000071525574, val_loss 6.856159210205078, val_accuracy 0.6687999963760376\n",
            "Epoch: 228, train_loss 6.4242119789123535, train_accuracy 0.6899999976158142, val_loss 6.793398380279541, val_accuracy 0.671999990940094\n",
            "Epoch: 229, train_loss 6.714337348937988, train_accuracy 0.6759999990463257, val_loss 6.867837905883789, val_accuracy 0.6685333251953125\n",
            "Epoch: 230, train_loss 6.631444454193115, train_accuracy 0.6800000071525574, val_loss 6.802199840545654, val_accuracy 0.6715999841690063\n",
            "Epoch: 231, train_loss 7.004462718963623, train_accuracy 0.6620000004768372, val_loss 6.727468967437744, val_accuracy 0.6751999855041504\n",
            "Epoch: 232, train_loss 6.507139682769775, train_accuracy 0.6859999895095825, val_loss 6.854485988616943, val_accuracy 0.6687999963760376\n",
            "Epoch: 233, train_loss 6.631444454193115, train_accuracy 0.6800000071525574, val_loss 6.841667652130127, val_accuracy 0.6694666743278503\n",
            "Epoch: 234, train_loss 7.626160621643066, train_accuracy 0.6320000290870667, val_loss 6.9404473304748535, val_accuracy 0.6649333238601685\n",
            "Epoch: 235, train_loss 7.377481937408447, train_accuracy 0.6439999938011169, val_loss 6.955085754394531, val_accuracy 0.6642666459083557\n",
            "Epoch: 236, train_loss 6.797230243682861, train_accuracy 0.671999990940094, val_loss 6.822455406188965, val_accuracy 0.6706666946411133\n",
            "Epoch: 237, train_loss 7.418928623199463, train_accuracy 0.6420000195503235, val_loss 6.893186092376709, val_accuracy 0.6670666933059692\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for step, (batch_X, batch_Y) in enumerate(train_data.take(steps_per_epoch), 1):\n",
        "        run_optimizer(batch_X, batch_Y)\n",
        "\n",
        "    val_pred = model(valid_features)\n",
        "    val_loss = cross_entropy(val_pred, valid_labels)\n",
        "    val_acc = accuracy(val_pred, valid_labels)\n",
        "    train_pred = model(batch_X)\n",
        "    train_loss = cross_entropy(train_pred, batch_Y)\n",
        "    train_acc = accuracy(train_pred, batch_Y)\n",
        "    manager.save()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, train_loss {train_loss}, train_accuracy {train_acc}, val_loss {val_loss}, val_accuracy {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89964352-f865-4417-afa5-26bfef13f437",
      "metadata": {
        "id": "89964352-f865-4417-afa5-26bfef13f437"
      },
      "outputs": [],
      "source": [
        "test_pred = model(test_features)\n",
        "test_loss = cross_entropy(test_pred, test_labels)\n",
        "test_acc = accuracy(test_pred, test_labels)\n",
        "print(f\"Test_loss {test_loss}, Test_accuracy {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbaa8601-7cee-442a-a667-c1e849ca20ce",
      "metadata": {
        "id": "cbaa8601-7cee-442a-a667-c1e849ca20ce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}